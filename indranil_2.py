# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WajIFcQljMzO18pyRTy8HqFYPNWGMLXl
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.ticker as mticker

import re
from pathlib import Path

import warnings
warnings.filterwarnings('ignore')

# Local file path
path = 'OASI Research Demographics 0708 final - stats copy new.xlsx'
df3 = pd.read_excel(path, sheet_name = '3C TEAR final')
df3.drop(['Complications', 'Epaq PF issued', 'Epaq recieved'], axis = 1 , inplace = True)

print(df3.head())
print(df3.columns)

from collections import defaultdict

# 1) Lire avec 2 lignes d'en-tête -> MultiIndex de colonnes
df4_raw = pd.read_excel(path, sheet_name='4th degree tear final', header=[0, 1])

df4_raw.drop(['EPAQ PF issued', 'EPAQ PF received'], axis = 1 , inplace = True)

print(df4_raw.head())
print(df4_raw.columns)

# 2) Fonction de nettoyage des libellés
def clean_lbl(x):
    if x is None:
        return ''
    s = str(x).strip()
    if s.lower().startswith('unnamed'):
        return ''
    return s

# 3) Aplatir en "Top_Sub" si les deux existent, sinon prendre ce qui existe
flat_cols = []
for top, sub in df4_raw.columns:
    top_c = clean_lbl(top)
    sub_c = clean_lbl(sub)
    if top_c and sub_c:
        name = f"{top_c}_{sub_c}"
    elif top_c:
        name = top_c
    elif sub_c:
        name = sub_c
    else:
        name = "Unnamed"

    flat_cols.append(name)

# 4) Rendre les noms uniques si doublons (ajoute .1, .2, …)
seen = defaultdict(int)
uniq_cols = []
for name in flat_cols:
    if name in seen:
        seen[name] += 1
        uniq_cols.append(f"{name}.{seen[name]}")
    else:
        seen[name] = 0
        uniq_cols.append(name)

# 5) Appliquer et récupérer le DataFrame final (les 2 lignes d'en-tête ne sont PAS dans les données)
df4 = df4_raw.copy()
df4.columns = uniq_cols


# df4.columns = df4.columns.str.replace(r'^Colostomy_', '', regex=True)
df4 = df4.dropna(axis=1, how='all')







# --- Q1 (updated): Stratification by Age (Groups: <20, 20–30, 30–40, >40) ---

# Define bins and labels (right=False -> [lower, upper))
bins = [-np.inf, 20, 30, 40, np.inf]
labels = ['<20', '20-30', '30-40', '>40']

# Ensure age is numeric and categorize
df3['Age_Group'] = pd.cut(pd.to_numeric(df3['Age_at_deliveryDate'], errors='coerce'),
                          bins=bins, labels=labels, right=False)
df4['Age_Group'] = pd.cut(pd.to_numeric(df4['Age_at_deliveryDate'], errors='coerce'),
                          bins=bins, labels=labels, right=False)

# Relative frequencies (%) for each dataset (among known ages)
freq3 = df3['Age_Group'].value_counts(normalize=True).reindex(labels) * 100
freq4 = df4['Age_Group'].value_counts(normalize=True).reindex(labels) * 100

age_freq_table = pd.DataFrame({
    '3C TEAR final (%)': freq3.round(2),
    '4th degree tear final (%)': freq4.round(2)
}).fillna(0)

print("Age distribution (%)")
print(age_freq_table)

# Optional: report missing age share
miss3 = df3['Age_Group'].isna().mean() * 100
miss4 = df4['Age_Group'].isna().mean() * 100
print(f"\nMissing age group: 3C = {miss3:.2f}%, 4th = {miss4:.2f}%")

# --- Stratification into Ethnicity Groups & Relative Frequencies ---

# Calculate relative frequencies (percentage) for each dataset
ethnicity_freq3 = df3['EthnicOrigin'].value_counts(normalize=True) * 100
ethnicity_freq4 = df4['EthnicOrigin'].value_counts(normalize=True) * 100

# Round for cleaner display
ethnicity_freq3 = ethnicity_freq3.round(2)
ethnicity_freq4 = ethnicity_freq4.round(2)

# Convert indices to strings before combining
ethnicity_freq3.index = ethnicity_freq3.index.astype(str)
ethnicity_freq4.index = ethnicity_freq4.index.astype(str)

# Combine into one DataFrame for reporting
ethnicity_table = pd.DataFrame({
    '3C TEAR final (%)': ethnicity_freq3,
    '4th degree tear final (%)': ethnicity_freq4
}).fillna(0)

print("\nEthnicity distribution (%)")
print(ethnicity_table)

# --- Q3 (final): Stratification by Parity (Groups: <1, 1, 2, >2) ---


# 1) Ensure Parity is numeric
df3['Parity_num'] = pd.to_numeric(df3['Parity'], errors='coerce')
df4['Parity_num'] = pd.to_numeric(df4['Parity'], errors='coerce')

# 2) Create Parity groups: <1, 1, 2, >2
df3['Parity_Group_v2'] = np.select(
    [df3['Parity_num'] < 1, df3['Parity_num'] == 1, df3['Parity_num'] == 2, df3['Parity_num'] > 2],
    ['<1', '1', '2', '>2'],
    default=None
)
df4['Parity_Group_v2'] = np.select(
    [df4['Parity_num'] < 1, df4['Parity_num'] == 1, df4['Parity_num'] == 2, df4['Parity_num'] > 2],
    ['<1', '1', '2', '>2'],
    default=None
)

# 3) Percentages over ALL records (denominator = total N)
order = ['<1', '1', '2', '>2']
n3, n4 = len(df3), len(df4)

counts3_all = df3['Parity_Group_v2'].value_counts().reindex(order).fillna(0)
counts4_all = df4['Parity_Group_v2'].value_counts().reindex(order).fillna(0)

pct3_all = (counts3_all / n3) * 100
pct4_all = (counts4_all / n4) * 100

parity_table_v2_all = pd.DataFrame({
    '3C TEAR final (%) (over all records)': pct3_all.round(2),
    '4th degree tear final (%) (over all records)': pct4_all.round(2)
})

print("Parity distribution (%): <1, 1, 2, >2 — over ALL records")
print(parity_table_v2_all)



# --- Q4: Stratification by BMI (Groups: <25, 25-30, 30-40, >40) ---

# Ensure BMI is numeric
df3['BMI_val'] = pd.to_numeric(df3['bmi'], errors='coerce')
df4['BMI_val'] = pd.to_numeric(df4['bmi'], errors='coerce')

# Define bins and labels
bins = [-np.inf, 25, 30, 40, np.inf]     # [<25), [25,30), [30,40), [40, +inf)
labels = ['<25', '25-30', '30-40', '>40']

# Categorize
df3['BMI_Group'] = pd.cut(df3['BMI_val'], bins=bins, labels=labels, right=False)
df4['BMI_Group'] = pd.cut(df4['BMI_val'], bins=bins, labels=labels, right=False)

# Relative frequencies (%) for each dataset (among known BMI)
order = labels
bmi_freq3 = df3['BMI_Group'].value_counts(normalize=True).reindex(order) * 100
bmi_freq4 = df4['BMI_Group'].value_counts(normalize=True).reindex(order) * 100

# Combine and display
bmi_table = pd.DataFrame({
    '3C TEAR final (%)': bmi_freq3.round(2),
    '4th degree tear final (%)': bmi_freq4.round(2)
}).fillna(0)

print("\nBMI distribution (%)")
print(bmi_table)

# Optional: show missing BMI share for transparency (doesn't change the table above)
miss3 = df3['BMI_Group'].isna().mean() * 100
miss4 = df4['BMI_Group'].isna().mean() * 100
print(f"\nMissing BMI: 3C = {miss3:.2f}%, 4th = {miss4:.2f}%")

# --- Q5: Stratification by Baby Birthweight (Groups: <2.5, 2.5–4, >4 kg) ---

# Ensure baby weight is numeric and convert to kg if given in grams
df3['Baby_weight_num'] = pd.to_numeric(df3['Baby_weight'], errors='coerce')
df4['Baby_weight_num'] = pd.to_numeric(df4['Baby_weight'], errors='coerce')

# If values look like grams (>20), convert to kg
df3['Baby_weight_kg'] = np.where(df3['Baby_weight_num'] > 20,
                                 df3['Baby_weight_num'] / 1000.0,
                                 df3['Baby_weight_num'])
df4['Baby_weight_kg'] = np.where(df4['Baby_weight_num'] > 20,
                                 df4['Baby_weight_num'] / 1000.0,
                                 df4['Baby_weight_num'])

# Create birthweight groups using explicit conditions for clean boundaries
# (<2.5), (2.5–4.0), (>4.0)
df3['BW_Group'] = np.select(
    [df3['Baby_weight_kg'] < 2.5,
     (df3['Baby_weight_kg'] >= 2.5) & (df3['Baby_weight_kg'] <= 4.0),
     df3['Baby_weight_kg'] > 4.0],
    ['<2.5', '2.5-4', '>4'],
    default=None
)

df4['BW_Group'] = np.select(
    [df4['Baby_weight_kg'] < 2.5,
     (df4['Baby_weight_kg'] >= 2.5) & (df4['Baby_weight_kg'] <= 4.0),
     df4['Baby_weight_kg'] > 4.0],
    ['<2.5', '2.5-4', '>4'],
    default=None
)

# Relative frequencies (%) among known weights
order = ['<2.5', '2.5-4', '>4']
bw_freq3 = df3['BW_Group'].value_counts(normalize=True).reindex(order) * 100
bw_freq4 = df4['BW_Group'].value_counts(normalize=True).reindex(order) * 100

bw_table = pd.DataFrame({
    '3C TEAR final (%)': bw_freq3.round(2),
    '4th degree tear final (%)': bw_freq4.round(2)
}).fillna(0)

print("\nBaby birthweight distribution (%)")
print(bw_table)

# Optional transparency on missing/undetermined
miss3_bw = df3['BW_Group'].isna().mean() * 100
miss4_bw = df4['BW_Group'].isna().mean() * 100
print(f"\nUndetermined/ Missing birthweight: 3C = {miss3_bw:.2f}%, 4th = {miss4_bw:.2f}%")

# --- Q6 (all categories): Value counts (%) for Mode of Delivery over ALL rows ---

# 1) Normalize raw strings
df3['MOD_UP'] = df3['MOD'].astype(str).str.strip().str.upper()
df4['MOD_UP'] = df4['MOD'].astype(str).str.strip().str.upper()

# 2) Start from normalized value, then map common aliases to canonical labels.
#    Unmatched non-empty strings will stay as-is (so you see ALL categories).
df3['MOD_ALL'] = df3['MOD_UP']
df4['MOD_ALL'] = df4['MOD_UP']

# SVD / NVD / VD synonyms
svd_mask_3 = df3['MOD_UP'].isin({'SVD','NVD','VD','NORMAL VAGINAL DELIVERY','SPONTANEOUS VAGINAL DELIVERY','VAGINAL'})
svd_mask_4 = df4['MOD_UP'].isin({'SVD','NVD','VD','NORMAL VAGINAL DELIVERY','SPONTANEOUS VAGINAL DELIVERY','VAGINAL'})
df3.loc[svd_mask_3, 'MOD_ALL'] = 'SVD'
df4.loc[svd_mask_4, 'MOD_ALL'] = 'SVD'

# Ventouse / vacuum / Kiwi
vent_mask_3 = df3['MOD_UP'].str.contains(r'VENTOUSE|VACUUM|KIWI', na=False)
vent_mask_4 = df4['MOD_UP'].str.contains(r'VENTOUSE|VACUUM|KIWI', na=False)
df3.loc[vent_mask_3, 'MOD_ALL'] = 'VENTOUSE'
df4.loc[vent_mask_4, 'MOD_ALL'] = 'VENTOUSE'

# Kielland forceps (KF / KFD / KIELLAND)
kiel_mask_3 = df3['MOD_UP'].str.contains(r'KIELLAND|^KF$|^KFD$', na=False)
kiel_mask_4 = df4['MOD_UP'].str.contains(r'KIELLAND|^KF$|^KFD$', na=False)
df3.loc[kiel_mask_3, 'MOD_ALL'] = 'KIELLAND FORCEPS'
df4.loc[kiel_mask_4, 'MOD_ALL'] = 'KIELLAND FORCEPS'

# NBFD (non-Kielland forceps): explicit NBFD or any FORCEPS not already mapped to Kielland
nbfd_mask_3 = (df3['MOD_UP'].eq('NBFD')) | (df3['MOD_UP'].str.contains('FORCEPS', na=False) & (~kiel_mask_3))
nbfd_mask_4 = (df4['MOD_UP'].eq('NBFD')) | (df4['MOD_UP'].str.contains('FORCEPS', na=False) & (~kiel_mask_4))
df3.loc[nbfd_mask_3, 'MOD_ALL'] = 'NBFD'
df4.loc[nbfd_mask_4, 'MOD_ALL'] = 'NBFD'

# CS / Caesarean synonyms (if present)
cs_mask_3 = df3['MOD_UP'].str.contains(r'\bCS\b|C/S|CAESAREAN|CESAREAN|EMCS|ELCS', na=False)
cs_mask_4 = df4['MOD_UP'].str.contains(r'\bCS\b|C/S|CAESAREAN|CESAREAN|EMCS|ELCS', na=False)
df3.loc[cs_mask_3, 'MOD_ALL'] = 'CS'
df4.loc[cs_mask_4, 'MOD_ALL'] = 'CS'

# 3) Label blanks as 'MISSING' so they're included in the denominator
df3.loc[df3['MOD_ALL'].eq('') | df3['MOD_ALL'].isin(['NAN','NONE']), 'MOD_ALL'] = 'MISSING'
df4.loc[df4['MOD_ALL'].eq('') | df4['MOD_ALL'].isin(['NAN','NONE']), 'MOD_ALL'] = 'MISSING'

# 4) Value counts over ALL rows (percentages)
mod_pct_3 = (df3['MOD_ALL'].value_counts(dropna=False, normalize=True) * 100).round(2)
mod_pct_4 = (df4['MOD_ALL'].value_counts(dropna=False, normalize=True) * 100).round(2)

# 5) Combine side-by-side (outer join keeps ALL categories from both datasets)
mod_table_all = pd.DataFrame({
    '3C TEAR final (%)': mod_pct_3,
    '4th degree tear final (%)': mod_pct_4
}).fillna(0).sort_index()

print("\nMode of Delivery — ALL categories (% of ALL records, with 'MISSING' included):")
print(mod_table_all)





# --- Q7: Stratification by Episiotomy within Mode of Delivery (8 groups) ---
# Groups required:
#   SVD with episiotomy / SVD without episiotomy
#   NBFD with episiotomy / NBFD without episiotomy
#   Ventouse with episiotomy / Ventouse without episiotomy
#   Kielland forceps with episiotomy / Kielland forceps without episiotomy
#
# Percentages are over ALL records (denominator = total N), and we also report
# the remainder share that falls outside these combos (e.g., CS/unknown/missing).



# ---------- 1) Normalize Mode of Delivery ----------
df3['MOD_UP'] = df3['MOD'].fillna('').astype(str).str.strip().str.upper()
df4['MOD_UP'] = df4['MOD'].fillna('').astype(str).str.strip().str.upper()

# SVD / NVD / VD synonyms
svd_aliases = {'SVD','NVD','VD','NORMAL VAGINAL DELIVERY','SPONTANEOUS VAGINAL DELIVERY','VAGINAL'}
svd_3 = df3['MOD_UP'].isin(svd_aliases)
svd_4 = df4['MOD_UP'].isin(svd_aliases)

# Ventouse / vacuum / Kiwi
vent_3 = df3['MOD_UP'].str.contains(r'VENTOUSE|VACUUM|KIWI', na=False)
vent_4 = df4['MOD_UP'].str.contains(r'VENTOUSE|VACUUM|KIWI', na=False)

# Kielland forceps (KF / KFD / KIELLAND)
kiel_3 = df3['MOD_UP'].str.contains(r'KIELLAND|^KF$|^KFD$', na=False)
kiel_4 = df4['MOD_UP'].str.contains(r'KIELLAND|^KF$|^KFD$', na=False)

# NBFD (non-Kielland forceps): explicit NBFD OR any FORCEPS not already Kielland
nbfd_3 = (df3['MOD_UP'].eq('NBFD')) | (df3['MOD_UP'].str.contains('FORCEPS', na=False) & (~kiel_3))
nbfd_4 = (df4['MOD_UP'].eq('NBFD')) | (df4['MOD_UP'].str.contains('FORCEPS', na=False) & (~kiel_4))

df3['MOD_Group'] = np.select(
    [svd_3, nbfd_3, vent_3, kiel_3],
    ['SVD', 'NBFD', 'Ventouse', 'Kielland forceps'],
    default=None
)
df4['MOD_Group'] = np.select(
    [svd_4, nbfd_4, vent_4, kiel_4],
    ['SVD', 'NBFD', 'Ventouse', 'Kielland forceps'],
    default=None
)

# ---------- 2) Normalize Episiotomy ----------
# Note: df3 column is 'Episiotomy'; df4 column is 'Episotomy' (as in your sheet)
df3['EPI_UP'] = df3['Episiotomy'].fillna('').astype(str).str.strip().str.upper()
df4['EPI_UP'] = df4['Episotomy'].fillna('').astype(str).str.strip().str.upper()

df3['Epi_Flag'] = np.where(df3['EPI_UP'].isin({'YES','Y'}), 'with episiotomy',
                   np.where(df3['EPI_UP'].isin({'NO','N'}), 'without episiotomy', None))
df4['Epi_Flag'] = np.where(df4['EPI_UP'].isin({'YES','Y'}), 'with episiotomy',
                   np.where(df4['EPI_UP'].isin({'NO','N'}), 'without episiotomy', None))


# ---------- 3) Build the 8 combination labels ----------
df3['MOD_EPI'] = np.where(df3['MOD_Group'].notna() & df3['Epi_Flag'].notna(),
                          df3['MOD_Group'].astype(str) + ' ' + df3['Epi_Flag'].astype(str), None)
df4['MOD_EPI'] = np.where(df4['MOD_Group'].notna() & df4['Epi_Flag'].notna(),
                          df4['MOD_Group'].astype(str) + ' ' + df4['Epi_Flag'].astype(str), None)


order = [
    'SVD with episiotomy', 'SVD without episiotomy',
    'NBFD with episiotomy', 'NBFD without episiotomy',
    'Ventouse with episiotomy', 'Ventouse without episiotomy',
    'Kielland forceps with episiotomy', 'Kielland forceps without episiotomy'
]

# ---------- 4) Percentages over ALL records ----------
n3, n4 = len(df3), len(df4)

pct3 = (df3['MOD_EPI'].value_counts().reindex(order).fillna(0) / n3) * 100
pct4 = (df4['MOD_EPI'].value_counts().reindex(order).fillna(0) / n4) * 100

epi_by_mod_table = pd.DataFrame({
    '3C TEAR final (%)': pct3.round(2),
    '4th degree tear final (%)': pct4.round(2)
})

print("\nEpisiotomy within Mode of Delivery (% of ALL records):")
print(epi_by_mod_table)

# ---------- 5) Remainder outside specified combos (CS/unknown/missing) ----------
rem3 = 100 - epi_by_mod_table['3C TEAR final (%)'].sum()
rem4 = 100 - epi_by_mod_table['4th degree tear final (%)'].sum()
print(f"\nOutside specified combos or missing MOD/Episiotomy: 3C = {rem3:.2f}%, 4th = {rem4:.2f}%")



# --- Q8: Percentage of patients who had previous 3rd-degree tear ---

# 1) Normalize raw values
df3['prev_oasi_raw'] = df3['Prev OASI'].astype(str).str.strip().str.upper()
df4['prev_oasi_raw'] = df4['Previous OASI'].astype(str).str.strip().str.upper()

# 2) Map to Yes/No/NaN
# Notes:
# - Treat 'NOA' as NO (your df4 counts show 'NOA' is the most common "no" code)
# - Treat '1xSVD' as NO (indicates prior delivery type, not prior OASI)
yes_vals = {'Y', 'YES', 'TRUE', '1'}
no_vals  = {'N', 'NO', 'NOA', 'N ', '1XSVD'}  # include 1xSVD as NO

df3['Prev_OASI_Flag'] = np.where(df3['prev_oasi_raw'].isin(yes_vals), 'Yes',
                          np.where(df3['prev_oasi_raw'].isin(no_vals), 'No', None))
df4['Prev_OASI_Flag'] = np.where(df4['prev_oasi_raw'].isin(yes_vals), 'Yes',
                          np.where(df4['prev_oasi_raw'].isin(no_vals), 'No', None))

# Among KNOWN values only (drops NaNs from denominator)
pct_yes_3_known = (df3['Prev_OASI_Flag'].value_counts(normalize=True).get('Yes', 0) * 100)
pct_yes_4_known = (df4['Prev_OASI_Flag'].value_counts(normalize=True).get('Yes', 0) * 100)

print(f"\nPrevious 3rd-degree tear (% among KNOWN values): "
      f"3C = {pct_yes_3_known:.2f}%, 4th = {pct_yes_4_known:.2f}%")

# 1.Age_at_deliveryDate for df3 and df4 but we created Age_Group: Groups: <20, 20–30, 30–40, >40
# 2. Parity_num for df3 and df4 but we created Parity_Group_v2: Groups: <1, 1, 2, >2
# 3.EthnicOrigin for df3 and df4. maybe we need to groyp categpries together ?
#   3C TEAR final (%)  4th degree tear final (%)
# EthnicOrigin
# 0                                           7.55                       3.96
# ANY OTHER ASIAN BACKGROUND                  2.83                       1.98
# ANY OTHER BLACK BACKGROUND                  0.94                       0.00
# ANY OTHER ETHNIC GROUP                      3.77                       7.92
# ANY OTHER MIXED BACKGROUND                  0.94                       0.99
# ANY OTHER WHITE BACKGROUND                  5.66                       3.96
# BANGLADESHI                                 0.94                       2.97
# BLACK AFRICAN                               3.77                       9.90
# CHINESE                                     1.89                       0.99
# INDIAN                                      5.66                       2.97
# MIXED WHITE & BLACK CARIBBEAN               0.00                       0.99
# NOT STATED                                  0.94                       4.95
# PAKISTANI                                   9.43                       8.91
# WHITE BRITISH                              54.72                      48.51
# WHITE IRISH                                 0.94                       0.991.
# 4. Baby_weight_num for df3 and df4 but we created BW_Group Groups: <2.5, 2.5–4, >4 kg
# 5. Episiotomy: df3 column is 'Episiotomy'; df4 column is 'Episotomy' but we created Epi_Flag: 'with episiotomy' and 'without episiotomy'
# 6. repair type:  df3['Repair ']  df4['Method of repair EAS']. but created Repair_Type
# 7. MOD for df3 and df4 but 75% of missing values in df3 , 0 in df4. created mod_all feature
# Mode of Delivery — ALL categories (% of ALL records, with 'MISSING' included):
#                       3C TEAR final (%)  4th degree tear final (%)
# MOD_ALL
# INSTRUMENTAL IN ROOM               0.94                       0.00
# KIELLAND FORCEPS                   0.00                       2.97
# MISSING                           75.47                       0.00
# NBFD                               7.55                      33.66
# SVD                               13.21                      54.46
# VENTOUSE                           2.83                       8.91

# --- Q9: Type of repair (Overlapping / End to end) with 'Missing' category ---
# Columns:
#   df3 -> 'Repair '     (note the trailing space)
#   df4 -> 'Method of repair EAS'
# Output: % over ALL records, with categories: Overlapping, End to end, Other/Unclear, Missing

import re

# ---------- df3 ----------
raw3 = df3['Repair ']
raw3_str = raw3.astype(str).str.strip()
missing3 = raw3.isna() | raw3_str.eq('')

# normalize for matching (uppercase + remove non-letters)
norm3 = raw3_str.str.upper().str.replace(r'[^A-Z]', '', regex=True)

is_overlap_3 = norm3.str.contains('OVERLAP', na=False)
is_e2e_3     = norm3.str.contains('ENDTOEND', na=False)

df3['Repair_Type'] = np.select(
    [
        ~missing3 & is_overlap_3,
        ~missing3 & is_e2e_3,
        ~missing3 & ~(is_overlap_3 | is_e2e_3)
    ],
    [
        'Overlapping',
        'End to end',
        'Other/Unclear'     # e.g., 'Button hole', '??', etc.
    ],
    default='Missing'        # true missing/blank
)

# ---------- df4 ----------
raw4 = df4['Method of repair EAS']
raw4_str = raw4.astype(str).str.strip()
missing4 = raw4.isna() | raw4_str.eq('')

norm4 = raw4_str.str.upper().str.replace(r'[^A-Z]', '', regex=True)

is_overlap_4 = norm4.str.contains('OVERLAP', na=False)   # catches 'OVERLAPPING', 'OVERLAPPPING', 'OVerlapping'
is_e2e_4     = norm4.str.contains('ENDTOEND', na=False)  # catches 'End to end', 'End to end ?'

df4['Repair_Type'] = np.select(
    [
        ~missing4 & is_overlap_4,
        ~missing4 & is_e2e_4,
        ~missing4 & ~(is_overlap_4 | is_e2e_4)
    ],
    [
        'Overlapping',
        'End to end',
        'Other/Unclear'     # e.g., 'Button hole', 'Buttonhole', 'Button hole tear'
    ],
    default='Missing'
)

# ---------- Percentages over ALL records ----------
order = ['Overlapping', 'End to end', 'Other/Unclear', 'Missing']

pct3 = (df3['Repair_Type'].value_counts(dropna=False, normalize=True)
          .reindex(order).fillna(0) * 100).round(2)
pct4 = (df4['Repair_Type'].value_counts(dropna=False, normalize=True)
          .reindex(order).fillna(0) * 100).round(2)

repair_table = pd.DataFrame({
    '3C TEAR final (%)': pct3,
    '4th degree tear final (%)': pct4
})

print("\nType of repair (% of ALL records):")
print(repair_table)

# === Q10 — Subsequent deliveries (FINAL TABLE; ALL categories incl. Missing/Unknown; denom = total N) ===
# Categories:
#   - CS
#   - VD without subsequent OASI
#   - VD/NBFD with subsequent OASI
#   - No subsequent pregnancy
#   - Missing/Unknown
# Assumes: df3 and df4 already exist, and df4 is already flattened.

# ---------------------- df3: classify from Notes (exclude "Prev/Previous/Prior CS") ----------------------
notes3 = df3['Notes'].astype(str).str.upper().str.strip()

# delivery year for index event (if column missing, all NaN -> no post-index check)
if 'deliveryDate' in df3.columns:
    index_year3 = pd.to_datetime(df3['deliveryDate'], errors='coerce').dt.year
else:
    index_year3 = pd.Series([np.nan] * len(df3), index=df3.index)

# Patterns
cs_pat3    = r'(?:^|\W)(?:\d+\s*X\s*)?CS(?:\W|$)|C/S|CAESAR|CESAR|EMCS|ELCS|CAESAREAN|CESAREAN|CS\s*\d{4}'
vd_pat3    = r'(?:^|\W)(?:\d+\s*X\s*)?(?:S?VD)(?:\W|$)|VAGINAL|VENTOUSE|VACUUM|KIWI|FORCEPS|NBFD|KIELLAND'
oasi_pat3  = r'\b(3A|3B|3C|3RD|4TH|OASI)\b|3RD\s*DEGREE|3\s*RD\s*DEG'

cs_3   = notes3.str.contains(cs_pat3,   na=False)
vd_3   = notes3.str.contains(vd_pat3,   na=False)
oasi_3 = notes3.str.contains(oasi_pat3, na=False)

# "Prev/prior" markers to avoid misclassifying prior history as subsequent
ante_mask3  = notes3.str.contains(r'\bPREV(?:IOUS)?\b|PRIOR|HX\b', na=False)
prev_cs_mask = notes3.str.contains(
    r'(?:PREV(?:IOUS)?|PRIOR|HX)\s*[^A-Z0-9]*CS|CS\s*[^A-Z0-9]*(?:PREV(?:IOUS)?|PRIOR|HX)', na=False
)

# Year extraction to infer post-index mention
years_list3 = notes3.str.findall(r'(?:19|20)\d{2}')
years_num3  = years_list3.apply(lambda lst: [int(x) for x in lst] if isinstance(lst, list) else [])
max_year3   = years_num3.apply(lambda lst: max(lst) if lst else np.nan)
any_future3 = (max_year3 > index_year3).fillna(False)

# Treat as "subsequent" only if not labeled previous OR there is an explicit post-index year
subseq_hint3 = (~ante_mask3) | any_future3

# Administrative/ambiguous notes -> Missing
ambiguous3 = notes3.str.contains(
    r'NOT\s+DOCUMENTED|REPAIR TECHNIQUE|TOO\s+FAR\s+BACK|LORE?NZO|'
    r'LOST\s+TO\s+FU|\?\s*LOST\s*TO\s*FU|POOR\s+COMPLIANCE|DNAD|DID\s+NOT\s+ATTEND|FAILED\s+TO\s+ATTEND|'
    r'NOT\s+COMPLETED|TOO\s+SOON\s+AFTER\s+DELIVERY|NO\s+FOLLOW\s+UP\s+LETTERS|'
    r'ASYMPTOMATIC|DECLINED\s+FURTHER\s+IX|DECLINED\s+IX|FISTULA',
    na=False
)

# Explicit "no subsequent pregnancy" phrases (rare in df3, but handle if present)
no_subseq3 = notes3.str.contains(
    r'NO\s+SUBSEQUENT|NO\s+FURTHER\s+(?:PREGNANCY|DELIVER(?:Y|IES))|NO\s+MORE\s+PREGNANC(?:Y|IES)|NONE\s+AFTER',
    na=False
)

# Final flags conditioned by subseq hint
cs_subseq3   = cs_3   & (~prev_cs_mask) & subseq_hint3
vd_subseq3   = vd_3   & subseq_hint3
oasi_subseq3 = oasi_3 & subseq_hint3

df3['Q10_Final'] = np.nan
df3.loc[no_subseq3, 'Q10_Final'] = 'No subsequent pregnancy'
df3.loc[df3['Q10_Final'].isna() & cs_subseq3, 'Q10_Final'] = 'CS'
df3.loc[df3['Q10_Final'].isna() & (vd_subseq3 & oasi_subseq3), 'Q10_Final'] = 'VD/NBFD with subsequent OASI'
df3.loc[df3['Q10_Final'].isna() & (vd_subseq3 & ~oasi_subseq3), 'Q10_Final'] = 'VD without subsequent OASI'
# If OASI mentioned but mode not explicit (and passes subseq hint) -> VD + OASI
df3.loc[df3['Q10_Final'].isna() & oasi_subseq3, 'Q10_Final'] = 'VD/NBFD with subsequent OASI'
# Force Missing/Unknown for purely administrative notes that remain unclassified
df3.loc[df3['Q10_Final'].isna() & ambiguous3, 'Q10_Final'] = 'Missing/Unknown'
df3['Q10_Final'] = df3['Q10_Final'].fillna('Missing/Unknown')

# ---------------------- df4: use structured fields ----------------------
sub4  = df4['Subsequent'].astype(str).str.upper().str.strip()
mod4  = df4['Future pregnancy_MOD'].astype(str).str.upper().str.strip()
tear4 = df4['Future pregnancy_Tear (if applicable)'].astype(str).str.upper().str.strip()

# Harmonize small whitespace, e.g. 'N ' -> 'N'
tear4 = tear4.str.replace(r'\s+', '', regex=True)

cs_pat   = r'(?:^|\W)(?:\d+\s*X\s*)?CS(?:\W|$)|C/S|CAESAR|CESAR|EMCS|ELCS|CAESAREAN|CESAREAN|CS\s*\d{4}'
vd_pat   = r'(?:^|\W)(?:\d+\s*X\s*)?(?:S?VD)(?:\W|$)|VENTOUSE|VACUUM|KIWI|FORCEPS|NBFD|KIELLAND|VAGINAL'
oasi_pat = r'\b(3A|3B|3C|3RD|4TH|OASI)\b'

sub_has_cs   = sub4.str.contains(cs_pat,   na=False)
sub_has_vd   = sub4.str.contains(vd_pat,   na=False)
sub_has_oasi = sub4.str.contains(oasi_pat, na=False)

mod_has_cs = mod4.str.contains(cs_pat, na=False)
mod_has_vd = mod4.str.contains(vd_pat, na=False)
mod_none   = mod4.eq('NONE')

tear_oasi = tear4.isin({'3A','3B','3C','3RD','4TH','Y'})

any_cs   = sub_has_cs | mod_has_cs
any_vd   = sub_has_vd | mod_has_vd
any_oasi = sub_has_oasi | tear_oasi

# Explicit "No subsequent pregnancy"
no_subseq4 = mod_none | sub4.eq('N')

df4['Q10_Final'] = np.nan
df4.loc[no_subseq4, 'Q10_Final'] = 'No subsequent pregnancy'
df4.loc[df4['Q10_Final'].isna() & any_cs, 'Q10_Final'] = 'CS'
df4.loc[df4['Q10_Final'].isna() & (any_vd & any_oasi), 'Q10_Final'] = 'VD/NBFD with subsequent OASI'
df4.loc[df4['Q10_Final'].isna() & (any_vd & ~any_oasi), 'Q10_Final'] = 'VD without subsequent OASI'
df4['Q10_Final'] = df4['Q10_Final'].fillna('Missing/Unknown')

# ---------------------- Final single table (counts + % of ALL) ----------------------
order = ['CS', 'VD without subsequent OASI', 'VD/NBFD with subsequent OASI', 'No subsequent pregnancy', 'Missing/Unknown']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q10_Final'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q10_Final'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

q10_single_table = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("\nQ10 — Subsequent deliveries (ALL categories incl. Missing/Unknown); denominator = total N")
print(q10_single_table)



# --- Q11: Type of repair (simple, total-N denominator, all categories + Missing) ---


# -------- df3 (clean already): 'Repair ' has only 'Overlapping' and 'End to end', no missing --------
df3['Q11_Repair'] = df3['Repair '].astype(str).str.strip()
# Guard (in case of any unexpected value)
valid3 = df3['Q11_Repair'].isin(['Overlapping', 'End to end'])
df3.loc[~valid3, 'Q11_Repair'] = 'Other/Unclear'  # should be none
# No missing in df3 per your note; if any appear, mark:
df3['Q11_Repair'] = df3['Q11_Repair'].replace({'': 'Missing'})


# -------- df4: normalize typos/variants into Overlapping / End to end / Other/Unclear / Missing --------
raw4 = df4['Method of repair EAS']
raw4_str = raw4.astype(str).str.strip()
is_missing4 = raw4.isna() | raw4_str.eq('')

# Simple normalization: uppercase and remove non-letters
norm4 = raw4_str.str.upper().str.replace(r'[^A-Z]', '', regex=True)

is_overlap_4 = norm4.str.contains('OVERLAP', na=False)   # catches OVerlapping / Overlappping / ' Overlapping'
is_e2e_4     = norm4.str.contains('ENDTOEND', na=False)  # catches 'End to end' and 'End to end ?'

df4['Q11_Repair'] = np.select(
    [
        ~is_missing4 & is_overlap_4,
        ~is_missing4 & is_e2e_4,
        ~is_missing4 & ~(is_overlap_4 | is_e2e_4)
    ],
    [
        'Overlapping',
        'End to end',
        'Other/Unclear'   # e.g., Button hole / Buttonhole / ??
    ],
    default='Missing'      # true missing/blank
)

# -------- Final table: counts + % over ALL rows (includes Missing) --------
order = ['Overlapping', 'End to end', 'Other/Unclear', 'Missing']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q11_Repair'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q11_Repair'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

repair_table_q11 = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("Q11 — Type of repair (ALL categories incl. Missing); denominator = total N")
print(repair_table_q11)



# --- Q12: Percentage receiving Laxatives (ALL categories + Missing; denom = total N) ---

# Normalize raw values
lx3_raw = df3['Laxatives'].astype(str).str.strip().str.upper()
lx4_raw = df4['Laxatives'].astype(str).str.strip().str.upper()

yes_vals = {'YES','Y','TRUE','1'}
no_vals  = {'NO','N','FALSE','0'}

df3['Q12_Lax'] = np.where(lx3_raw.isin(yes_vals), 'Yes',
                   np.where(lx3_raw.isin(no_vals), 'No', 'Missing/Unknown'))
df4['Q12_Lax'] = np.where(lx4_raw.isin(yes_vals), 'Yes',
                   np.where(lx4_raw.isin(no_vals), 'No', 'Missing/Unknown'))

# Final table (counts + % over ALL rows)
order = ['Yes', 'No', 'Missing/Unknown']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q12_Lax'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q12_Lax'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

q12_table = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("Q12 — Laxatives given in post-op advice (ALL categories incl. Missing/Unknown); denominator = total N")
print(q12_table)

# --- Q13: Percentage receiving Antibiotics (ALL categories + Missing; denom = total N) ---



# Normalize raw values
abx3_raw = df3['Antibiotics '].astype(str).str.strip().str.upper()  # note trailing space in df3 column
abx4_raw = df4['Antibiotics'].astype(str).str.strip().str.upper()

yes_vals = {'YES','Y','TRUE','1'}
no_vals  = {'NO','N','FALSE','0'}

df3['Q13_Abx'] = np.where(abx3_raw.isin(yes_vals), 'Yes',
                   np.where(abx3_raw.isin(no_vals), 'No', 'Missing/Unknown'))
df4['Q13_Abx'] = np.where(abx4_raw.isin(yes_vals), 'Yes',
                   np.where(abx4_raw.isin(no_vals), 'No', 'Missing/Unknown'))

# Final table (counts + % over ALL rows)
order = ['Yes', 'No', 'Missing/Unknown']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q13_Abx'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q13_Abx'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

q13_table = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("Q13 — Antibiotics given in post-op advice (ALL categories incl. Missing/Unknown); denominator = total N")
print(q13_table)





# --- Q14: Seen subsequently in perineal clinic (ALL categories + Missing; denom = total N) ---


# Normalize raw values
pc3_raw = df3['Clinic f/up'].astype(str).str.strip().str.upper()
pc4_raw = df4['Perineal clinic'].astype(str).str.strip().str.upper()

# Mapping sets
yes_vals = {'YES','Y','SEEN'}
no_vals  = {'NO','N'}
dna_vals = {'DNA'}  # did not attend

# Map to unified categories
df3['Q14_Clinic'] = np.where(pc3_raw.isin(yes_vals), 'Seen',
                      np.where(pc3_raw.isin(no_vals), 'Not seen', 'Missing/Unknown'))

df4['Q14_Clinic'] = np.where(pc4_raw.isin(yes_vals), 'Seen',
                      np.where(pc4_raw.isin(no_vals), 'Not seen',
                      np.where(pc4_raw.isin(dna_vals), 'DNA', 'Missing/Unknown')))

# Final table (counts + % over ALL rows)
order = ['Seen', 'Not seen', 'DNA', 'Missing/Unknown']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q14_Clinic'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q14_Clinic'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

q14_table = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("Q14 — Seen in perineal clinic (ALL categories incl. Missing/Unknown); denominator = total N")
print(q14_table)



# --- Q15: Subsequent Endoanal scanning (EA physiology) ---
# Single table with ALL categories + Missing/Unknown; denominator = TOTAL N
# Columns:
#   df3 -> 'Endoanal physiology'
#   df4 -> 'EA physiology'

# -------- df3 mapping --------
raw3 = df3['Endoanal physiology']
s3 = raw3.astype(str).str.replace('\xa0', ' ', regex=False).str.strip().str.upper()

dna3 = s3.str.contains(r'\bDNA\b|DECLINED', na=False)
done3 = s3.str.startswith('YES') | s3.str.contains(r'\bDEFECT\b|\bINTACT\b|\bNORMAL\b|\bEAS\b|\bIAS\b', na=False)
notdone3 = s3.str.fullmatch(r'NO|NO\.', na=False) | s3.eq('NO')

df3['Q15_EA'] = np.select(
    [dna3, done3, notdone3],
    ['DNA/Declined', 'Done', 'Not done'],
    default='Missing/Unknown'
)

# -------- df4 mapping --------
raw4 = df4['EA physiology']
s4 = raw4.astype(str).str.replace('\xa0', ' ', regex=False).str.strip().str.upper()

dna4 = s4.str.contains(r'\bDNA\b|DECLINED', na=False)
done4 = s4.str.contains(r'\bDEFECT\b|\bINTACT\b|\bNORMAL\b|\bEAS\b|\bIAS\b', na=False)
notdone4 = s4.str.fullmatch(r'NO|NO\.', na=False) | s4.eq('NO')

df4['Q15_EA'] = np.select(
    [dna4, done4, notdone4],
    ['DNA/Declined', 'Done', 'Not done'],
    default='Missing/Unknown'
)

# -------- Final table (counts + % over ALL rows) --------
order = ['Done', 'Not done', 'DNA/Declined', 'Missing/Unknown']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q15_EA'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q15_EA'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

q15_table = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("Q15 — Subsequent Endoanal scanning (ALL categories incl. Missing/Unknown); denominator = total N")
print(q15_table)

# --- Q16: Patients having complications (Y at column V) ---
# Uses:
#   df3['Complications']          (may be all NaN in your file)
#   df4['Complication']           (values: 'Y','N', NaN)
# Output: single table with ALL categories + Missing/Unknown; denominator = total N



yes_vals = {'Y','YES','TRUE','1'}
no_vals  = {'N','NO','FALSE','0'}

# ---- df3 ----
if 'Complications' in df3.columns:
    c3_raw = df3['Complications'].astype(str).str.strip().str.upper()
    df3['Q16_Comp'] = np.where(c3_raw.isin(yes_vals), 'Yes',
                        np.where(c3_raw.isin(no_vals), 'No', 'Missing/Unknown'))
else:
    # if the column was dropped earlier, mark all as Missing/Unknown
    df3['Q16_Comp'] = 'Missing/Unknown'

# ---- df4 ----
c4_raw = df4['Complication'].astype(str).str.strip().str.upper()
df4['Q16_Comp'] = np.where(c4_raw.isin(yes_vals), 'Yes',
                    np.where(c4_raw.isin(no_vals), 'No', 'Missing/Unknown'))

# ---- Final table (counts + % over ALL rows) ----
order = ['Yes', 'No', 'Missing/Unknown']
n3, n4 = len(df3), len(df4)

counts3 = df3['Q16_Comp'].value_counts().reindex(order).fillna(0).astype(int)
pct3    = (counts3 / n3 * 100).round(2)

counts4 = df4['Q16_Comp'].value_counts().reindex(order).fillna(0).astype(int)
pct4    = (counts4 / n4 * 100).round(2)

q16_table = pd.DataFrame({
    '3C count': counts3,
    '3C % (of all)': pct3,
    '4th count': counts4,
    '4th % (of all)': pct4
})

print("Q16 — Complications (Y at column V) — ALL categories incl. Missing/Unknown; denominator = total N")
print(q16_table)





# --- Q17 (df4 only): Types of complications (ALL categories + Missing/Unknown; denom = total N) ---

# Source column (df4 only)
t = df4['Complication types']

# Normalize & handle missing
s = t.astype(str).str.strip()
is_missing = t.isna() | s.eq('')
s = s.str.upper()

# Light typo fixes / harmonization (keep it simple)
s = s.str.replace(r'\bFAEC', 'FEC', regex=True)        # FAECAL -> FECAL
s = s.str.replace(r'FLATAL|FLATAS', 'FLATUS', regex=True)
s = s.str.replace('INCONTINECE', 'INCONTINENCE', regex=False)
s = s.str.replace('INCOMPLTE', 'INCOMPLETE', regex=False)
s = s.str.replace('DEFEACATION', 'DEFECATION', regex=False)
s = s.str.replace('ASYMPTOMAQTIC', 'ASYMPTOMATIC', regex=False)
s = s.str.replace('ASYMTOMATIC', 'ASYMPTOMATIC', regex=False)
s = s.str.replace('UNKNOOWN', 'UNKNOWN', regex=False)
s = s.str.replace(r'\bRVF\b', 'RECTOVAGINAL FISTULA', regex=True)
s = s.str.replace('URGENCY OF STOOL', 'FECAL URGENCY', regex=False)
s = s.str.replace('URGENCY OF BOWELS', 'FECAL URGENCY', regex=False)
s = s.str.replace('TRANSIET', 'TRANSIENT', regex=False)

# Apply Missing/Unknown (also collapse literal 'UNKNOWN')
s = s.where(~is_missing, 'Missing/Unknown').replace({'UNKNOWN': 'Missing/Unknown'})

df4['Q17_Type'] = s

# Frequencies (% of ALL)
freq4 = (df4['Q17_Type'].value_counts(normalize=True) * 100).round(2)

# Put Missing/Unknown at the bottom
order = [c for c in freq4.index if c != 'Missing/Unknown']
if 'Missing/Unknown' in freq4.index:
    order += ['Missing/Unknown']

q17_df4_table = pd.DataFrame({'4th % (of all)': freq4.reindex(order)})

print("\nQ17 — Types of complications (df4 only; ALL categories incl. Missing/Unknown); denominator = total N")
print(q17_df4_table)

# --- Q18 (df4 only): Further therapy among those with complications ---
# Columns used: 'Complication', 'Biofeedback', 'Sphincteroplasty', 'SNS', 'Colostomy'

# 1) Filter to patients WITH complications
comp_mask = df4['Complication'].astype(str).str.strip().str.upper().eq('Y')
df4_comp = df4.loc[comp_mask].copy()
den = len(df4_comp)  # denominator = number with complications

# 2) Helper to map messy values -> categories
def map_bio(x):
    if pd.isna(x) or str(x).strip()=='':
        return 'Missing/Unknown'
    s = str(x).strip().upper()
    if 'Y' in s:
        return 'Yes'              # includes 'Y + PTNS'
    if s in {'N','NO'}:
        return 'No'
    if 'DNA' in s or 'DECLINED' in s:
        return 'DNA/Declined'
    if 'AWAIT' in s or 'CONSIDER' in s:
        return 'Considered/Awaiting'
    return 'Other/Unclear'

def map_sph(x):
    if pd.isna(x) or str(x).strip()=='':
        return 'Missing/Unknown'
    s = str(x).strip().upper()
    if s.startswith('Y'):
        return 'Yes'              # 'Y   ', 'Y (REPAIR OF RVF'
    if 'AWAIT' in s:
        return 'Considered/Awaiting'
    if s in {'N','NO'}:
        return 'No'
    if 'DNA' in s or 'DECLINED' in s:
        return 'DNA/Declined'
    return 'Other/Unclear'

def map_sns(x):
    if pd.isna(x) or str(x).strip()=='':
        return 'Missing/Unknown'
    s = str(x).strip().upper()
    if s in {'N','NO'}:
        return 'No'
    if 'AWAIT' in s or 'CONSIDER' in s:
        return 'Considered/Awaiting'
    if s.startswith('Y'):
        return 'Yes'
    if 'DNA' in s or 'DECLINED' in s:
        return 'DNA/Declined'
    return 'Other/Unclear'

def map_col(x):
    if pd.isna(x) or str(x).strip()=='':
        return 'Missing/Unknown'
    s = str(x).strip().upper()
    if 'Y' in s:
        return 'Yes'              # e.g., 'Y + REVERSAL'
    if s.endswith('- N') or s in {'N','NO'}:
        return 'No'               # e.g., 'SIGMOIDOSCOPY - N'
    if 'DNA' in s or 'DECLINED' in s:
        return 'DNA/Declined'
    if 'AWAIT' in s or 'CONSIDER' in s:
        return 'Considered/Awaiting'
    return 'Other/Unclear'        # e.g., 'RVF REPAIR' (not a colostomy)

# 3) Apply mappings
cats = ['Yes','No','Considered/Awaiting','DNA/Declined','Other/Unclear','Missing/Unknown']

bio = df4_comp['Biofeedback'].map(map_bio)
sph = df4_comp['Sphincteroplasty'].map(map_sph)
sns = df4_comp['SNS'].map(map_sns)
col = df4_comp['Colostomy'].map(map_col)

# 4) Build % table (denominator = # with complications)
tbl = pd.DataFrame({
    'Biofeedback % (among complications)': (bio.value_counts(normalize=True).reindex(cats).fillna(0) * 100).round(2),
    'Sphincter repair % (among complications)': (sph.value_counts(normalize=True).reindex(cats).fillna(0) * 100).round(2),
    'SNS % (among complications)': (sns.value_counts(normalize=True).reindex(cats).fillna(0) * 100).round(2),
    'Colostomy % (among complications)': (col.value_counts(normalize=True).reindex(cats).fillna(0) * 100).round(2),
})

print(f"\nQ18 — Further therapy among those with complications (df4 only; n={den})")
print(tbl)







# --- PART TWO / Q1 (df4 only): Bowel — Immediate issues ---
# We report ALL categories (Yes / No / Missing) with denominator = TOTAL N.
# Columns used in df4:
#   - 'Bowel (immediate issues)_Faecal incontinence'
#   - 'Bowel (immediate issues)_Flatus incontinence'
#   - 'Bowel (immediate issues)_Urgency of stool'
#   - 'Bowel (immediate issues)_Able to defer bowels?'


# Column names
c_faecal = 'Bowel (immediate issues)_Faecal incontinence'
c_flatus = 'Bowel (immediate issues)_Flatus incontinence'
c_urg    = 'Bowel (immediate issues)_Urgency of stool'
c_defer  = 'Bowel (immediate issues)_Able to defer bowels?'


# Helper to map Y/N/NaN -> Yes/No/Missing
def yn_to_cat(series):
    s = series.astype(str).str.strip().str.upper()
    return np.where(s.eq('Y'), 'Yes',
            np.where(s.eq('N'), 'No', 'Missing/Unknown'))

df4['Q2_1_Faecal'] = yn_to_cat(df4[c_faecal])
df4['Q2_1_Flatus'] = yn_to_cat(df4[c_flatus])
df4['Q2_1_Urg']    = yn_to_cat(df4[c_urg])
df4['Q2_1_Defer']  = yn_to_cat(df4[c_defer])

cats = ['Yes','No','Missing/Unknown']
N = len(df4)

# Build % table (rows = categories; columns = the four immediate-issue items)
q1_immediate_df4 = pd.DataFrame({
    'Faecal incontinence % (of all)': (df4['Q2_1_Faecal'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Flatus incontinence % (of all)': (df4['Q2_1_Flatus'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Urgency % (of all)':             (df4['Q2_1_Urg'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Able to defer bowels % (of all)':(df4['Q2_1_Defer'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
}, index=cats)

print("Q1 — Bowel (Immediate issues), df4 only — ALL categories incl. Missing/Unknown; denominator = total N =", N)
print(q1_immediate_df4)

# --- PART TWO / Q2 (df4 only): Bowel — LONG TERM issues + persistence from immediate ---
# We compute, for each item:
#   - LT prevalence % (of ALL patients)  -> denominator = total N
#   - Persistence % (conditional)        -> among LT=Y, % with IM=Y
# Special case H ("Able to defer"): the symptom is INABILITY to defer,
#   so persistence is among LT = No, % with IM = No.

N = len(df4)

# ----- Columns -----
im_faecal = df4['Bowel (immediate issues)_Faecal incontinence'].astype(str).str.strip().str.upper()
lt_faecal = df4['Bowel (long term issues)_Faecal incontinence'].astype(str).str.strip().str.upper()

im_flatus = df4['Bowel (immediate issues)_Flatus incontinence'].astype(str).str.strip().str.upper()
lt_flatus = df4['Bowel (long term issues)_Flatus incontinence'].astype(str).str.strip().str.upper()

im_urg    = df4['Bowel (immediate issues)_Urgency of stool'].astype(str).str.strip().str.upper()
lt_urg    = df4['Bowel (long term issues)_Urgency'].astype(str).str.strip().str.upper()

im_defer  = df4['Bowel (immediate issues)_Able to defer bowels?'].astype(str).str.strip().str.upper()
lt_defer  = df4['Bowel (long term issues)_Able to defer bowels?'].astype(str).str.strip().str.upper()

# Only count valid IM values for persistence denominators
valid_im = {'Y','N'}

# ----- E) Faecal incontinence -----
lt_yes = (lt_faecal == 'Y').sum()
prev_E = round(lt_yes / N * 100, 2)

den_E = ((lt_faecal == 'Y') & im_faecal.isin(valid_im)).sum()
num_E = ((lt_faecal == 'Y') & (im_faecal == 'Y')).sum()
persist_E = round(num_E / den_E * 100, 2) if den_E > 0 else np.nan

# ----- F) Flatus incontinence -----
lt_yes = (lt_flatus == 'Y').sum()
prev_F = round(lt_yes / N * 100, 2)

den_F = ((lt_flatus == 'Y') & im_flatus.isin(valid_im)).sum()
num_F = ((lt_flatus == 'Y') & (im_flatus == 'Y')).sum()
persist_F = round(num_F / den_F * 100, 2) if den_F > 0 else np.nan

# ----- G) Urgency -----
lt_yes = (lt_urg == 'Y').sum()
prev_G = round(lt_yes / N * 100, 2)

den_G = ((lt_urg == 'Y') & im_urg.isin(valid_im)).sum()
num_G = ((lt_urg == 'Y') & (im_urg == 'Y')).sum()
persist_G = round(num_G / den_G * 100, 2) if den_G > 0 else np.nan

# ----- H) Able to defer bowels?  (symptom = INABILITY to defer -> 'N') -----
# Report prevalence of INABILITY at LT (No), and persistence among those with LT=No (IM=No)
lt_no = (lt_defer == 'N').sum()
prev_H_symptom = round(lt_no / N * 100, 2)   # % cannot defer at long term (symptom prevalence)

den_H = ((lt_defer == 'N') & im_defer.isin(valid_im)).sum()
num_H = ((lt_defer == 'N') & (im_defer == 'N')).sum()
persist_H = round(num_H / den_H * 100, 2) if den_H > 0 else np.nan

# (Optionnel) si tu veux aussi le % "peut différer" à LT:
lt_yes_defer = (lt_defer == 'Y').sum()
prev_H_can_defer = round(lt_yes_defer / N * 100, 2)

# ----- Assemble table -----
q2_bowel_lt = pd.DataFrame({
    'LT prevalence % (of all)': [
        prev_E,
        prev_F,
        prev_G,
        prev_H_symptom
    ],
    'Persistence % (conditional)': [
        persist_E,
        persist_F,
        persist_G,
        persist_H
    ]
}, index=[
    'Faecal incontinence',
    'Flatus incontinence',
    'Urgency',
    'Inability to defer bowels (symptom)'
])

print(f"Q2 — Bowel (LONG TERM) — df4 only. N={N}")
print(q2_bowel_lt)

# (Optionnel) afficher aussi le % "Able to defer (Yes) at LT" pour information:
print(f"\nInfo: LT 'Able to defer' (Yes) % of all = {prev_H_can_defer:.2f}%")

# --- PART TWO / Q3 (df4 only): Urine — Immediate issues ---
# We report ALL categories (Yes / No / Missing) with denominator = TOTAL N.

N = len(df4)
cats = ['Yes','No','Missing/Unknown']

# Column names (df4)
c_urg   = 'Urinary problems (immediate issues)_Urgency'
c_freq  = 'Urinary problems (immediate issues)_Frequency'
c_leak  = 'Urinary problems (immediate issues)_Leakage'
c_sui   = 'Urinary problems (immediate issues)_Leakage on strenuous activity'
c_void  = 'Urinary problems (immediate issues)_Voiding dysfunction'

# Helper: map Y/N/NaN -> Yes/No/Missing
def yn_to_cat(series):
    s = series.astype(str).str.strip().str.upper()
    return np.where(s.eq('Y'), 'Yes', np.where(s.eq('N'), 'No', 'Missing/Unknown'))

df4['Q3_Urg']  = yn_to_cat(df4[c_urg])
df4['Q3_Freq'] = yn_to_cat(df4[c_freq])
df4['Q3_Leak'] = yn_to_cat(df4[c_leak])
df4['Q3_SUI']  = yn_to_cat(df4[c_sui])
df4['Q3_Void'] = yn_to_cat(df4[c_void])

# % table (rows = Yes/No/Missing; cols = 5 items)
q3_urine_immediate_df4 = pd.DataFrame({
    'Urgency % (of all)':            (df4['Q3_Urg'] .value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Frequency % (of all)':          (df4['Q3_Freq'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Leakage % (of all)':            (df4['Q3_Leak'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Leakage on strenuous % (of all)':(df4['Q3_SUI'] .value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Voiding dysfunction % (of all)':(df4['Q3_Void'].value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
}, index=cats)

print("Q3 — Urine (Immediate issues), df4 only — ALL categories incl. Missing/Unknown; denominator = total N =", N)
print(q3_urine_immediate_df4)

# --- PART TWO / Q4 (df4 only): Urine — LONG TERM issues + persistence from immediate ---
# For each item we compute:
#   - LT prevalence % (of ALL patients)        -> denominator = total N
#   - Persistence % (conditional on LT = Y)    -> among LT=Y, % with IM=Y (ignoring IM missing)

N = len(df4)
valid_im = {'Y','N'}

# ----- Column names -----
IM_URG  = 'Urinary problems (immediate issues)_Urgency'
LT_URG  = 'Urinary problems (long term issues)_Urgency'

IM_FREQ = 'Urinary problems (immediate issues)_Frequency'
LT_FREQ = 'Urinary problems (long term issues)_Frequency'

IM_LEAK = 'Urinary problems (immediate issues)_Leakage'
LT_LEAK = 'Urinary problems (long term issues)_Leakage'

IM_SUI  = 'Urinary problems (immediate issues)_Leakage on strenuous activity'
LT_SUI  = 'Urinary problems (long term issues)_Leakage on strenuous activity'

IM_VOID = 'Urinary problems (immediate issues)_Voiding dysfunction'
LT_VOID = 'Urinary problems (long term issues)_Voiding dysfunction'

# Normalize to uppercase strings
im_urg  = df4[IM_URG].astype(str).str.strip().str.upper()
lt_urg  = df4[LT_URG].astype(str).str.strip().str.upper()

im_freq = df4[IM_FREQ].astype(str).str.strip().str.upper()
lt_freq = df4[LT_FREQ].astype(str).str.strip().str.upper()

im_leak = df4[IM_LEAK].astype(str).str.strip().str.upper()
lt_leak = df4[LT_LEAK].astype(str).str.strip().str.upper()

im_sui  = df4[IM_SUI].astype(str).str.strip().str.upper()
lt_sui  = df4[LT_SUI].astype(str).str.strip().str.upper()

im_void = df4[IM_VOID].astype(str).str.strip().str.upper()
lt_void = df4[LT_VOID].astype(str).str.strip().str.upper()

def lt_prev_and_persist(lt_ser, im_ser):
    lt_yes = (lt_ser == 'Y').sum()
    prev = round(lt_yes / N * 100, 2)
    den = ((lt_ser == 'Y') & im_ser.isin(valid_im)).sum()
    num = ((lt_ser == 'Y') & (im_ser == 'Y')).sum()
    persist = round(num / den * 100, 2) if den > 0 else np.nan
    return prev, persist

prev_U, pers_U = lt_prev_and_persist(lt_urg,  im_urg)
prev_F, pers_F = lt_prev_and_persist(lt_freq, im_freq)
prev_L, pers_L = lt_prev_and_persist(lt_leak, im_leak)
prev_S, pers_S = lt_prev_and_persist(lt_sui,  im_sui)
prev_V, pers_V = lt_prev_and_persist(lt_void, im_void)

q4_urine_lt = pd.DataFrame({
    'LT prevalence % (of all)': [
        prev_U, prev_F, prev_L, prev_S, prev_V
    ],
    'Persistence % (conditional)': [
        pers_U, pers_F, pers_L, pers_S, pers_V
    ]
}, index=[
    'Urgency',
    'Frequency',
    'Leakage',
    'Leakage on strenuous activity',
    'Voiding dysfunction'
])

print(f"\nQ4 — Urine (LONG TERM) — df4 only. N={N}")
print(q4_urine_lt)





# --- PART TWO / Q5 (df4 only): Vaginal issues — IMMEDIATE + persistence (for Dyspareunia & Vaginal lump) ---
# A) Immediate % with Body image issues (report Yes/No/Missing over ALL)
# B) Immediate % with Dyspareunia (ALL) + Persistence among IM=Y: % also LT=Y
# C) Immediate % with Vaginal lump (ALL) + Persistence among IM=Y: % also LT=Y


N = len(df4)
cats = ['Yes','No','Missing/Unknown']

# ----- Column names -----
IM_BODY = 'Vaginal problems (immediate)_Body image'
IM_DYSP = 'Vaginal problems (immediate)_Dyspareunia'
IM_LUMP = 'Vaginal problems (immediate)_Vaginal lump'

LT_BODY = 'Vaginal problems (long term)_Body image'
LT_DYSP = 'Vaginal problems (long term)_Dyspareunia'
LT_LUMP = 'Vaginal problems (long term)_Vaginal lump'


# Helper: map Y/N/NaN -> Yes/No/Missing
def yn_to_cat(series):
    s = series.astype(str).str.strip().str.upper()
    return np.where(s.eq('Y'), 'Yes', np.where(s.eq('N'), 'No', 'Missing/Unknown'))

# Normalize immediate and long-term
im_body = df4[IM_BODY].astype(str).str.strip().str.upper()
im_dysp = df4[IM_DYSP].astype(str).str.strip().str.upper()
im_lump = df4[IM_LUMP].astype(str).str.strip().str.upper()

lt_body = df4[LT_BODY].astype(str).str.strip().str.upper()
lt_dysp = df4[LT_DYSP].astype(str).str.strip().str.upper()
lt_lump = df4[LT_LUMP].astype(str).str.strip().str.upper()

# ----- Immediate distributions (Yes/No/Missing; denominator = TOTAL N) -----
df4['Q5_IM_Body'] = yn_to_cat(df4[IM_BODY])
df4['Q5_IM_Dysp'] = yn_to_cat(df4[IM_DYSP])
df4['Q5_IM_Lump'] = yn_to_cat(df4[IM_LUMP])

q5_immediate = pd.DataFrame({
    'Body image % (of all)': (pd.Series(df4['Q5_IM_Body']).value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Dyspareunia % (of all)': (pd.Series(df4['Q5_IM_Dysp']).value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
    'Vaginal lump % (of all)': (pd.Series(df4['Q5_IM_Lump']).value_counts(normalize=True).reindex(cats).fillna(0)*100).round(2),
}, index=cats)

print("Q5 — Vaginal issues (IMMEDIATE), df4 only — ALL categories incl. Missing/Unknown; denominator = total N =", N)
print(q5_immediate)

# ----- Persistence for Dyspareunia & Vaginal lump (among IM=Y) -----
valid_im = {'Y','N'}

# Dyspareunia: among IM=Y, % with LT=Y
den_dysp = (im_dysp == 'Y').sum()
num_dysp = ((im_dysp == 'Y') & (lt_dysp == 'Y')).sum()
persist_dysp = round(num_dysp / den_dysp * 100, 2) if den_dysp > 0 else np.nan

# Vaginal lump: among IM=Y, % with LT=Y
den_lump = (im_lump == 'Y').sum()
num_lump = ((im_lump == 'Y') & (lt_lump == 'Y')).sum()
persist_lump = round(num_lump / den_lump * 100, 2) if den_lump > 0 else np.nan

q5_persistence = pd.DataFrame({
    'Immediate prevalence % (of all)': [
        (im_dysp.eq('Y').sum() / N * 100).round(2),
        (im_lump.eq('Y').sum() / N * 100).round(2)
    ],
    'Persistence % (IM→LT | among IM=Y)': [
        persist_dysp,
        persist_lump
    ]
}, index=['Dyspareunia', 'Vaginal lump'])

print("\nQ5 — Persistence among those IMMEDIATE-positive (df4 only)")
print(q5_persistence)

# --- PART TWO / Q6 (df4 only): Vaginal issues — LONG TERM + persistence from immediate ---
# For each item (Body image, Dyspareunia, Vaginal lump) we compute:
#   - LT prevalence % (of ALL patients)        -> denominator = total N
#   - Persistence % (conditional on LT = Y)    -> among LT=Y, % with IM=Y (ignoring IM missing)


N = len(df4)
valid_im = {'Y','N'}

# Helper to pick the correct LT column name (some files use "...(long term issues)...")
def pick_col(primary, fallback):
    if primary in df4.columns:
        return primary
    if fallback in df4.columns:
        return fallback
    raise KeyError(f"Neither '{primary}' nor '{fallback}' found in df4.columns")

# ----- Column names -----
IM_BODY = 'Vaginal problems (immediate)_Body image'
IM_DYSP = 'Vaginal problems (immediate)_Dyspareunia'
IM_LUMP = 'Vaginal problems (immediate)_Vaginal lump'

LT_BODY = pick_col('Vaginal problems (long term issues)_Body image', 'Vaginal problems (long term)_Body image')
LT_DYSP = pick_col('Vaginal problems (long term issues)_Dyspareunia', 'Vaginal problems (long term)_Dyspareunia')
LT_LUMP = pick_col('Vaginal problems (long term issues)_Vaginal lump', 'Vaginal problems (long term)_Vaginal lump')

# Normalize to uppercase strings
im_body = df4[IM_BODY].astype(str).str.strip().str.upper()
lt_body = df4[LT_BODY].astype(str).str.strip().str.upper()

im_dysp = df4[IM_DYSP].astype(str).str.strip().str.upper()
lt_dysp = df4[LT_DYSP].astype(str).str.strip().str.upper()

im_lump = df4[IM_LUMP].astype(str).str.strip().str.upper()
lt_lump = df4[LT_LUMP].astype(str).str.strip().str.upper()

def lt_prev_and_persist(lt_ser, im_ser):
    lt_yes = (lt_ser == 'Y').sum()
    prev = round(lt_yes / N * 100, 2)
    den = ((lt_ser == 'Y') & im_ser.isin(valid_im)).sum()
    num = ((lt_ser == 'Y') & (im_ser == 'Y')).sum()
    persist = round(num / den * 100, 2) if den > 0 else np.nan
    return prev, persist

prev_BI, pers_BI = lt_prev_and_persist(lt_body, im_body)
prev_DY, pers_DY = lt_prev_and_persist(lt_dysp, im_dysp)
prev_LU, pers_LU = lt_prev_and_persist(lt_lump, im_lump)

q6_vaginal_lt = pd.DataFrame({
    'LT prevalence % (of all)': [
        prev_BI, prev_DY, prev_LU
    ],
    'Persistence % (conditional)': [
        pers_BI, pers_DY, pers_LU
    ]
}, index=[
    'Body image',
    'Dyspareunia',
    'Vaginal lump'
])

print(f"Q6 — Vaginal issues (LONG TERM) — df4 only. N={N}")
print(q6_vaginal_lt)



# 1.	Any relationship between maternal age, parity, ethnicity, baby birthweight, absence of episiotomy,
# type of repair and mode of delivery for having a future complication after 3c and 4th degree tear.
#   (Probably needs multiple regression both univariate and multivariate analysis – complications to be taken as binary data – Yes/No?
#   Can we use Likelihood ratio?)

# --- Create predictor feature: df4['Repair_Type'] ---
# Source column: 'Method of repair EAS'
# Categories: 'Overlapping' / 'End to end' / 'Other/Unclear' / NaN (missing left untouched)


raw = df4['Method of repair EAS']
s   = raw.astype(str).str.strip()

# Normalize for matching: UPPERCASE + keep letters only
norm = s.str.upper().str.replace(r'[^A-Z]', '', regex=True)

# Matching rules
is_overlap = norm.str.contains('OVERLAP',  na=False)   # ex: Overlapping / OVerlapping / Overlapp...
is_e2e     = norm.str.contains('ENDTOEND', na=False)   # ex: End to end / End-to-end

# Use np.select for classification
df4['Repair_Type'] = np.select(
    [
        is_overlap,
        is_e2e,
        ~(is_overlap | is_e2e)  # when it's neither overlap nor end-to-end
    ],
    [
        'Overlapping',
        'End to end',
        'Other/Unclear'
    ],
    default=None   # missing stays as NaN
)

# (Optional) Categorical type without forcing "Missing"
cats = ['Overlapping', 'End to end', 'Other/Unclear']
df4['Repair_Type'] = pd.Categorical(df4['Repair_Type'], categories=cats)

# --- Normalisation du Mode of Delivery pour df4 seulement ---

df4['MOD_UP'] = df4['MOD'].astype(str).str.strip().str.upper()
df4['MOD_ALL'] = df4['MOD_UP']

# SVD / NVD / VD / Vaginal
svd_mask = df4['MOD_UP'].isin({
    'SVD','NVD','VD',
    'NORMAL VAGINAL DELIVERY',
    'SPONTANEOUS VAGINAL DELIVERY',
    'VAGINAL'
})
df4.loc[svd_mask, 'MOD_ALL'] = 'SVD'

# Ventouse / Vacuum / Kiwi
vent_mask = df4['MOD_UP'].str.contains(r'VENTOUSE|VACUUM|KIWI', na=False)
df4.loc[vent_mask, 'MOD_ALL'] = 'VENTOUSE'

# Kielland forceps
kiel_mask = df4['MOD_UP'].str.contains(r'KIELLAND|^KF$|^KFD$', na=False)
df4.loc[kiel_mask, 'MOD_ALL'] = 'KIELLAND FORCEPS'

# NBFD (forceps sauf Kielland ou mention explicite)
nbfd_mask = (df4['MOD_UP'].eq('NBFD')) | (
    df4['MOD_UP'].str.contains('FORCEPS', na=False) & ~kiel_mask
)
df4.loc[nbfd_mask, 'MOD_ALL'] = 'NBFD'

# CS / Caesarean
cs_mask = df4['MOD_UP'].str.contains(r'\bCS\b|C/S|CAESAREAN|CESAREAN|EMCS|ELCS', na=False)
df4.loc[cs_mask, 'MOD_ALL'] = 'CS'



df4.loc[df4['MOD_ALL'].eq('') | df4['MOD_ALL'].isin(['NAN','NONE']), 'MOD_ALL'] = 'MISSING'

# 1. Age Num
#df4['Age_at_deliveryDate']

# 2. Parity group
df4['Parity_num'] = pd.to_numeric(df4['Parity'], errors='coerce')
df4['Parity_Group_v2'] = pd.cut(df4['Parity_num'],
                                   bins=[-1, 0, 1, 2, 100],
                                   labels=['<1', '1', '2', '>2'],
                                   right=True)

# 3. Ethnic group (collapse rare categories)
# Regroupement des ethnicités
def recode_ethnicity(x):
    if x in ["WHITE BRITISH", "ANY OTHER WHITE BACKGROUND", "WHITE IRISH"]:
        return "White"
    elif x in ["PAKISTANI", "BANGLADESHI", "INDIAN", "ANY OTHER ASIAN BACKGROUND"]:
        return "South Asian"
    elif x in ["BLACK AFRICAN"]:
        return "Black"
    elif x in ["ANY OTHER ETHNIC GROUP", "MIXED WHITE & BLACK CARIBBEAN",
               "ANY OTHER MIXED BACKGROUND", "CHINESE"]:
        return "Other/Mixed"
    elif x in ["NOT STATED", 0]:
        return "Unknown"
    else:
        return "Unknown"

df4["EthnicOrigin_cat"] = df4["EthnicOrigin"].apply(recode_ethnicity)

# 4. Baby weight group
#df['Baby_weight_num']

# 5."Absence_episiotomy"
df4["Absence_episiotomy"] = (df4["Episotomy"].str.strip().str.upper().map({"N": 1, "Y": 0, "YES": 0}))

# Nettoyage de la variable cible
df4['Complication_bin'] = df4['Complication'].map({'Yes': 1, 'No': 0})

# ==============================
# Q1 — df4 only: predictors + logistic regressions (uni & multi)
# ==============================

import statsmodels.formula.api as smf
from scipy.stats import chi2

# ---------- 1) Feature engineering EXACT per your spec (with small fixes) ----------

# (a) Repair_Type from 'Method of repair EAS' — leave missing as NaN
raw = df4['Method of repair EAS']
s   = raw.astype(str).str.strip()
norm = s.str.upper().str.replace(r'[^A-Z]', '', regex=True)

is_overlap = norm.str.contains('OVERLAP',  na=False)
is_e2e     = norm.str.contains('ENDTOEND', na=False)

df4['Repair_Type'] = np.select(
    [is_overlap, is_e2e, ~(is_overlap | is_e2e)],
    ['Overlapping', 'End to end', 'Other/Unclear'],
    default=None  # Changed np.nan to None
)
df4['Repair_Type'] = pd.Categorical(df4['Repair_Type'], categories=['Overlapping','End to end','Other/Unclear'])

# (b) MOD_ALL normalized (keep explicit 'MISSING' label)
df4['MOD_UP']  = df4['MOD'].astype(str).str.strip().str.upper()
df4['MOD_ALL'] = df4['MOD_UP']

svd_mask  = df4['MOD_UP'].isin({'SVD','NVD','VD','NORMAL VAGINAL DELIVERY','SPONTANEOUS VAGINAL DELIVERY','VAGINAL'})
vent_mask = df4['MOD_UP'].str.contains(r'VENTOUSE|VACUUM|KIWI', na=False)
kiel_mask = df4['MOD_UP'].str.contains(r'KIELLAND|^KF$|^KFD$', na=False)
nbfd_mask = (df4['MOD_UP'].eq('NBFD')) | (df4['MOD_UP'].str.contains('FORCEPS', na=False) & ~kiel_mask)
cs_mask   = df4['MOD_UP'].str.contains(r'\bCS\b|C/S|CAESAREAN|CESAREAN|EMCS|ELCS', na=False)

df4.loc[svd_mask,  'MOD_ALL'] = 'SVD'
df4.loc[vent_mask, 'MOD_ALL'] = 'VENTOUSE'
df4.loc[kiel_mask, 'MOD_ALL'] = 'KIELLAND FORCEPS'
df4.loc[nbfd_mask, 'MOD_ALL'] = 'NBFD'
df4.loc[cs_mask,   'MOD_ALL'] = 'CS'
df4.loc[df4['MOD_ALL'].eq('') | df4['MOD_ALL'].isin(['NAN','NONE']), 'MOD_ALL'] = 'MISSING'

# (c) Age (continuous): df4['Age_at_deliveryDate'] — already present

# (d) Parity group (fix df->df4)
df4['Parity_Group_v2'] = pd.cut(
    df4['Parity_num'],
    bins=[-1, 0, 1, 2, 100],
    labels=['<1', '1', '2', '>2'],
    right=True
)

# (e) Ethnic group (robust to case)
def recode_ethnicity(x):
    x = str(x).strip().upper()
    if x in ["WHITE BRITISH", "ANY OTHER WHITE BACKGROUND", "WHITE IRISH"]:
        return "White"
    elif x in ["PAKISTANI", "BANGLADESHI", "INDIAN", "ANY OTHER ASIAN BACKGROUND"]:
        return "South Asian"
    elif x in ["BLACK AFRICAN"]:
        return "Black"
    elif x in ["ANY OTHER ETHNIC GROUP", "MIXED WHITE & BLACK CARIBBEAN",
               "ANY OTHER MIXED BACKGROUND", "CHINESE"]:
        return "Other/Mixed"
    elif x in ["NOT STATED", "0"]:
        return "Unknown"
    else:
        return "Unknown"

df4["EthnicOrigin_cat"] = df4["EthnicOrigin"].apply(recode_ethnicity)

# (f) Baby weight (continuous): df4['Baby_weight_num'] — already present

# (g) Absence of episiotomy (binary 1/0), preserve NaN
df4["Absence_episiotomy"] = (
    df4["Episotomy"]
    .astype(str).str.strip().str.upper()
    .map({"N": 1, "Y": 0, "YES": 0})
)


df4['Baby_weight_num'] = pd.to_numeric(df4['Baby_weight'], errors='coerce')

# (h) Outcome: Complication_bin from Y/N (preserve NaN)
df4['Complication_bin'] = (
    df4['Complication']
    .astype(str).str.strip().str.upper()
    .map({'Y': 1, 'N': 0})
)

# ---------- 2) Logistic regressions (Robust implementation) ----------

import patsy
import statsmodels.api as sm
from scipy.stats import chi2

# Define predictors and outcome
predictors = [
    "Age_at_deliveryDate",
    "Baby_weight_num",
    "Absence_episiotomy",
    "Parity_Group_v2",
    "EthnicOrigin_cat",
    "Repair_Type",
    "MOD_ALL"
]
outcome = "Complication_bin"

# 0) Create a clean dataframe for the model
dfm = df4[[outcome] + predictors].copy()

# Keep rows with known outcome
dfm = dfm.dropna(subset=[outcome])

# Collapse ultra-rare MOD level to avoid 1-2 count columns that can cause instability
dfm['MOD_ALL'] = dfm['MOD_ALL'].replace({'KIELLAND FORCEPS': 'NBFD'})


# ---- Univariate models
univariate_results = {}
print("\n--- Running Univariate Regressions ---")
for var in predictors:
    # Create a model-specific dataframe that drops NaNs for the current predictor
    df_uni = dfm[[outcome, var]].dropna()

    if df_uni.empty:
        print(f"\n[WARN] No data for univariate model of {var} after dropping NaNs.")
        continue

    if df_uni[outcome].nunique() < 2:
        print(f"\n[WARN] Outcome is not binary for {var} after dropping NaNs. Skipping.")
        continue

    # Skip if predictor has no variation
    if df_uni[var].nunique() < 2 :
        print(f"\n[WARN] Predictor {var} has only one level after dropping NaNs. Skipping.")
        continue

    if df_uni[var].dtype.kind in "if":  # numeric/float/int
        formula = f"{outcome} ~ {var}"
    else:
        formula = f"{outcome} ~ C({var})"

    try:
        m = smf.logit(formula=formula, data=df_uni).fit(disp=0)
        univariate_results[var] = m
        print(f"\n--- Univariate: {var} ---")
        print(m.summary2().tables[1])
    except Exception as e:
        print(f"\n[WARN] Univariate failed for {var}: {e}")


# ---- Multivariable model
print("\n--- Running Multivariable Regression ---")

# 1) Show actual counts per level (helps see sparse/perfect levels)
for c in ['Parity_Group_v2','EthnicOrigin_cat','Repair_Type','MOD_ALL']:
    if c in dfm.columns:
        print(f"\n[{c}] value counts for multivariable model:")
        print(dfm[c].value_counts(dropna=False))

# 2) Build formula with explicit references (choose the most common levels as refs)
formula_full = (
    "Complication_bin ~ "
    "Age_at_deliveryDate + "
    "Baby_weight_num + "
    "Absence_episiotomy + "
    "C(Parity_Group_v2, Treatment(reference='1')) + "
    "C(EthnicOrigin_cat, Treatment(reference='White')) + "
    "C(Repair_Type, Treatment(reference='Overlapping')) + "
    "C(MOD_ALL, Treatment(reference='SVD'))"
)

# 3) Build design matrices; inspect rank & drop bad columns if needed
# Drop rows where ANY predictor is missing for the full model
dfm_full = dfm.dropna(subset=predictors)

if dfm_full[outcome].nunique() < 2:
    print("\n[ERROR] Not enough outcome variation in the data available for the full model. Cannot fit.")
else:
    try:
        y, X = patsy.dmatrices(formula_full, data=dfm_full, return_type='dataframe')

        # Drop all-zero columns (can happen if a level disappears after NA handling)
        zero_cols = [c for c in X.columns if (X[c].abs().sum() == 0)]
        if zero_cols:
            print("\nDropping all-zero columns:", zero_cols)
            X = X.drop(columns=zero_cols)

        # Drop duplicate columns (rare, but can occur if references collide)
        X = X.loc[:, ~X.columns.duplicated()]

        # Check rank
        rank = np.linalg.matrix_rank(X.values)
        if rank < X.shape[1]:
            print(f"\n[WARN] Design not full rank (rank={rank} < p={X.shape[1]}). Will attempt regularized fit if needed.")

        # 4) Try standard Logit first
        used_model = None
        regularized = False
        try:
            logit_model = sm.Logit(y, X).fit(disp=0)
            print("\n=== Multivariable Logistic Regression (standard MLE) ===")
            print(logit_model.summary2())
            used_model = logit_model
        except np.linalg.LinAlgError as e:
            print(f"\n[INFO] Standard MLE failed with singular matrix: {e}. Trying ridge-regularized logistic...")
            try:
                # Ridge regularization (L2): fit_regularized with L1_wt=0
                used_model = sm.Logit(y, X).fit_regularized(alpha=1.0, L1_wt=0.0, disp=0)
                regularized = True
                print("\n=== Multivariable Logistic Regression (regularized) ===")
                print("Note: p-values and CIs are not available for regularized models in statsmodels.")
                print(used_model.params)
            except Exception as e2:
                print(f"\n[ERROR] Regularized fit also failed: {e2}")

        if used_model:
            # 5) Likelihood Ratio Test vs null (only valid for standard MLE)
            if not regularized:
                null_m = sm.Logit(y, np.ones((len(y),1))).fit(disp=0)
                LR = 2 * (used_model.llf - null_m.llf)
                df_diff = int(X.shape[1] - 1)  # minus intercept
                p_lr = chi2.sf(LR, df_diff)
                print("\n=== Likelihood Ratio Test (full vs null) ===")
                print(f"LR Chi2 = {LR:.2f}, df = {df_diff}, p = {p_lr:.4g}")
            else:
                print("\n[Note] LRT is not directly comparable under regularization; interpret coefficients/ORs instead.")

            # 6) OR + 95% CI (works for standard MLE; for regularized, CI is not provided)
            if not regularized:
                params = used_model.params
                conf = used_model.conf_int()
                or_tab = pd.DataFrame({
                    'OR': np.exp(params),
                    'CI 2.5%': np.exp(conf[0]),
                    'CI 97.5%': np.exp(conf[1]),
                    'p (Wald)': used_model.pvalues
                })
                print("\n=== Adjusted Odds Ratios (multivariable) ===")
                print(or_tab)
            else:
                # Provide ORs only
                or_tab = pd.Series(np.exp(used_model.params), name='OR')
                print("\n=== Adjusted Odds Ratios (regularized; no CI) ===")
                print(or_tab)
    except patsy.PatsyError as e:
        print(f"\n[ERROR] Could not create design matrix, check categorical variable levels: {e}")


# =============================================================================
# PART TWO: Analytical Statistics (Specific Complications)
# =============================================================================

print("\n\n===================================================")
print("PART TWO: Analytical Statistics (Specific Complications)")
print("===================================================")

# --- Note on 3c Tear Data ---
print("\nNOTE: The following analyses are for 4th degree tears (df4) only, as the 3c tear data (df3)")
print("does not contain the detailed complication columns required for these models.")

# --- 1. Feature Engineering for Specific Complication Outcomes ---

# Helper to map Y/N -> 1.0/0.0
def yn_bool(series):
    s = series.astype(str).str.strip().str.upper()
    true_set  = {"Y","YES","1","TRUE"}
    false_set = {"N","NO","0","FALSE"}
    out = pd.Series(np.nan, index=s.index, dtype="float")
    out[s.isin(true_set)]  = 1.0
    out[s.isin(false_set)] = 0.0
    return out

# Helper to create 'any issue' flag (1 if any True, 0 if all observed are False)
def any_issue_from_flags(df_flags):
    any_true = df_flags.eq(1).any(axis=1)
    all_false_with_obs = (df_flags.eq(0).all(axis=1)) & df_flags.notna().any(axis=1)
    out = pd.Series(np.nan, index=df_flags.index, dtype="float")
    out[any_true] = 1.0
    out[all_false_with_obs] = 0.0
    return out

# a) Bowel Issues (Question 2)
bowel_im_cols = ['Bowel (immediate issues)_Faecal incontinence', 'Bowel (immediate issues)_Flatus incontinence', 'Bowel (immediate issues)_Urgency of stool']
bowel_lt_cols = ['Bowel (long term issues)_Faecal incontinence', 'Bowel (long term issues)_Flatus incontinence', 'Bowel (long term issues)_Urgency']
im_defer_issue = yn_bool(df4['Bowel (immediate issues)_Able to defer bowels?']).replace({1:0, 0:1}) # Invert, as N is the issue
lt_defer_issue = yn_bool(df4['Bowel (long term issues)_Able to defer bowels?']).replace({1:0, 0:1})

im_bowel_flags = df4[bowel_im_cols].apply(yn_bool)
im_bowel_flags['inability_to_defer'] = im_defer_issue
df4["Bowel_IM_any_issue"] = any_issue_from_flags(im_bowel_flags)

lt_bowel_flags = df4[bowel_lt_cols].apply(yn_bool)
lt_bowel_flags['inability_to_defer'] = lt_defer_issue
df4["Bowel_LT_any_issue"] = any_issue_from_flags(lt_bowel_flags)

# b) Urinary Issues (Question 3 & 4)
urinary_im_cols = ['Urinary problems (immediate issues)_Urgency', 'Urinary problems (immediate issues)_Frequency', 'Urinary problems (immediate issues)_Leakage', 'Urinary problems (immediate issues)_Leakage on strenuous activity', 'Urinary problems (immediate issues)_Voiding dysfunction']
urinary_lt_cols = ['Urinary problems (long term issues)_Urgency', 'Urinary problems (long term issues)_Frequency', 'Urinary problems (long term issues)_Leakage', 'Urinary problems (long term issues)_Leakage on strenuous activity', 'Urinary problems (long term issues)_Voiding dysfunction']

im_urine_flags = df4[urinary_im_cols].apply(yn_bool)
df4["Urine_IM_any_issue"] = any_issue_from_flags(im_urine_flags)

lt_urine_flags = df4[urinary_lt_cols].apply(yn_bool)
df4["Urine_LT_any_issue"] = any_issue_from_flags(lt_urine_flags)

# c) Vaginal Issues (Question 5)
vaginal_im_cols = ['Vaginal problems (immediate)_Body image', 'Vaginal problems (immediate)_Dyspareunia', 'Vaginal problems (immediate)_Vaginal lump']
vaginal_lt_cols = ['Vaginal problems (long term)_Body image', 'Vaginal problems (long term)_Dyspareunia', 'Vaginal problems (long term)_Vaginal lump']

im_vaginal_flags = df4[vaginal_im_cols].apply(yn_bool)
df4["Vaginal_IM_any_issue"] = any_issue_from_flags(im_vaginal_flags)

lt_vaginal_flags = df4[vaginal_lt_cols].apply(yn_bool)
df4["Vaginal_LT_any_issue"] = any_issue_from_flags(lt_vaginal_flags)

# --- 2. Reusable Function for Logistic Regression Analysis ---

def run_regression_analysis(df, outcome_var, predictors):
    print(f"\n\n--- REGRESSION ANALYSIS FOR: {outcome_var} ---")

    dfm = df[[outcome_var] + predictors].copy()
    dfm = dfm.dropna(subset=[outcome_var])

    if dfm.empty or dfm[outcome_var].nunique() < 2:
        print(f"[ERROR] Not enough data or outcome variation for {outcome_var}. Skipping.")
        return

    dfm["MOD_ALL"] = dfm["MOD_ALL"].replace({"KIELLAND FORCEPS": "NBFD"})
    
    formula = (
        f"{outcome_var} ~ "
        "Age_at_deliveryDate + Baby_weight_num + Absence_episiotomy + "
        "C(Parity_Group_v2, Treatment(reference='1')) + "
        "C(EthnicOrigin_cat, Treatment(reference='White')) + "
        "C(Repair_Type, Treatment(reference='Overlapping')) + "
        "C(MOD_ALL, Treatment(reference='SVD'))"
    )

    dfm_full = dfm.dropna(subset=predictors)

    if dfm_full.empty or dfm_full[outcome_var].nunique() < 2:
        print(f"[ERROR] Not enough data or outcome variation for {outcome_var} after dropping predictor NaNs. Skipping.")
        return

    try:
        y, X = patsy.dmatrices(formula, data=dfm_full, return_type='dataframe')
        
        # Drop all-zero columns and duplicates
        zero_cols = [c for c in X.columns if (X[c].abs().sum() == 0)]
        if zero_cols:
            print("\nDropping all-zero columns:", zero_cols)
            X = X.drop(columns=zero_cols)
        X = X.loc[:, ~X.columns.duplicated()]
        
        used_model = None
        regularized = False
        try:
            logit_model = sm.Logit(y, X).fit(disp=0)
            print("\n=== Multivariable Logistic Regression (standard MLE) ===")
            print(logit_model.summary2())
            used_model = logit_model
        except np.linalg.LinAlgError:
            print("\n[INFO] Standard MLE failed. Trying regularized logistic regression...")
            used_model = sm.Logit(y, X).fit_regularized(alpha=1.0, L1_wt=0.0, disp=0)
            regularized = True
            print("\n=== Multivariable Logistic Regression (regularized) ===")
            print("Note: p-values and CIs are not available for regularized models.")
            print(used_model.params)
        
        if used_model and not regularized:
            null_m = sm.Logit(y, np.ones((len(y),1))).fit(disp=0)
            LR = 2 * (used_model.llf - null_m.llf)
            df_diff = int(X.shape[1] - 1)
            p_lr = chi2.sf(LR, df_diff)
            print("\n=== Likelihood Ratio Test (full vs null) ===")
            print(f"LR Chi2 = {LR:.2f}, df = {df_diff}, p = {p_lr:.4g}")

            params = used_model.params
            conf = used_model.conf_int()
            or_tab = pd.DataFrame({
                'OR': np.exp(params),
                'CI 2.5%': np.exp(conf[0]),
                'CI 97.5%': np.exp(conf[1]),
                'p (Wald)': used_model.pvalues
            })
            print("\n=== Adjusted Odds Ratios (multivariable) ===")
            print(or_tab)

    except Exception as e:
        print(f"\n[ERROR] Could not run regression for {outcome_var}: {e}")

# --- 3. Run Analyses for Each Outcome ---

# Define the common set of predictors
analytic_predictors = [
    "Age_at_deliveryDate", "Baby_weight_num", "Absence_episiotomy",
    "Parity_Group_v2", "EthnicOrigin_cat", "Repair_Type", "MOD_ALL"
]

# Question 2
run_regression_analysis(df4, "Bowel_IM_any_issue", analytic_predictors)
run_regression_analysis(df4, "Bowel_LT_any_issue", analytic_predictors)

# Question 3 & 4
print("\n\nNOTE: Question 4 is a duplicate of Question 3. Running analysis for Urinary issues once.")
run_regression_analysis(df4, "Urine_IM_any_issue", analytic_predictors)
run_regression_analysis(df4, "Urine_LT_any_issue", analytic_predictors)

# Question 5
run_regression_analysis(df4, "Vaginal_IM_any_issue", analytic_predictors)
run_regression_analysis(df4, "Vaginal_LT_any_issue", analytic_predictors)


# =============================================================================
# PUBLICATION-READY TABLES AND VISUALIZATIONS
# =============================================================================

print("\n\n===================================================")
print("GENERATING PUBLICATION-READY OUTPUTS")
print("===================================================")

# --- 1. Demographic Characteristics Table (Table 1) ---
def create_demographic_table():
    """Create a publication-ready demographic characteristics table."""
    
    # Helper function to calculate stats
    def calc_stats(df, var, name):
        if df[var].dtype in ['object', 'category']:
            # Categorical variable
            counts = df[var].value_counts()
            total = len(df)
            result = []
            for cat, count in counts.items():
                pct = (count / total) * 100
                result.append(f"{count} ({pct:.1f}%)")
            return pd.Series(result, index=[f"{name} - {cat}" for cat in counts.index])
        else:
            # Continuous variable
            mean_val = df[var].mean()
            std_val = df[var].std()
            median_val = df[var].median()
            q25, q75 = df[var].quantile([0.25, 0.75])
            return pd.Series([
                f"{mean_val:.1f} ± {std_val:.1f}",
                f"{median_val:.1f} ({q25:.1f}-{q75:.1f})"
            ], index=[f"{name} - Mean ± SD", f"{name} - Median (IQR)"])
    
    # Create demographic table
    demo_vars = [
        ('Age_at_deliveryDate', 'Age (years)'),
        ('EthnicOrigin_cat', 'Ethnicity'),
        ('Parity_Group_v2', 'Parity'),
        ('BMI_val', 'BMI (kg/m²)'),
        ('Baby_weight_kg', 'Baby weight (kg)'),
        ('MOD_ALL', 'Mode of delivery'),
        ('Repair_Type', 'Repair type')
    ]
    
    table_data = []
    for var, name in demo_vars:
        if var in df3.columns:
            df3_stats = calc_stats(df3, var, name)
            table_data.append(pd.DataFrame({'3C Tears': df3_stats}))
        if var in df4.columns:
            df4_stats = calc_stats(df4, var, name)
            if table_data:
                table_data[-1]['4th Degree Tears'] = df4_stats
            else:
                table_data.append(pd.DataFrame({'4th Degree Tears': df4_stats}))
    
    demo_table = pd.concat(table_data).fillna('-')
    
    print("\n--- Table 1: Demographic Characteristics ---")
    print(demo_table.to_string())
    
    # Save to CSV
    demo_table.to_csv('Table1_Demographics.csv')
    print("\nSaved: Table1_Demographics.csv")
    
    return demo_table

# --- 2. Complications Summary Table (Table 2) ---
def create_complications_table():
    """Create a summary table of all complications."""
    
    complications_data = {
        'Complication Type': [
            'Any complication (general)',
            'Bowel - Immediate',
            'Bowel - Long-term', 
            'Urinary - Immediate',
            'Urinary - Long-term',
            'Vaginal - Immediate',
            'Vaginal - Long-term'
        ],
        '4th Degree Tears n (%)': []
    }
    
    # Calculate percentages for df4
    total_n = len(df4)
    
    # General complications
    general_comp = (df4['Complication_bin'] == 1).sum()
    complications_data['4th Degree Tears n (%)'].append(f"{general_comp} ({general_comp/total_n*100:.1f}%)")
    
    # Specific complications
    for comp_type in ['Bowel_IM_any_issue', 'Bowel_LT_any_issue', 'Urine_IM_any_issue', 
                      'Urine_LT_any_issue', 'Vaginal_IM_any_issue', 'Vaginal_LT_any_issue']:
        if comp_type in df4.columns:
            comp_count = (df4[comp_type] == 1).sum()
            valid_n = df4[comp_type].notna().sum()
            complications_data['4th Degree Tears n (%)'].append(f"{comp_count} ({comp_count/valid_n*100:.1f}%)")
        else:
            complications_data['4th Degree Tears n (%)'].append('N/A')
    
    comp_table = pd.DataFrame(complications_data)
    
    print("\n--- Table 2: Complications Summary ---")
    print(comp_table.to_string(index=False))
    
    # Save to CSV
    comp_table.to_csv('Table2_Complications.csv', index=False)
    print("\nSaved: Table2_Complications.csv")
    
    return comp_table

# --- 3. Regression Results Summary Table (Table 3) ---
def create_regression_summary_table():
    """Create a consolidated regression results table."""
    
    # Define predictors for the table
    predictor_names = {
        'Age_at_deliveryDate': 'Age (years)',
        'Baby_weight_num': 'Baby weight (kg)',
        'Absence_episiotomy': 'No episiotomy',
        "C(Parity_Group_v2, Treatment(reference='1'))[T.<1]": 'Parity <1 (vs 1)',
        "C(Parity_Group_v2, Treatment(reference='1'))[T.2]": 'Parity 2 (vs 1)',
        "C(EthnicOrigin_cat, Treatment(reference='White'))[T.South Asian]": 'South Asian (vs White)',
        "C(EthnicOrigin_cat, Treatment(reference='White'))[T.Black]": 'Black (vs White)',
        "C(EthnicOrigin_cat, Treatment(reference='White'))[T.Other/Mixed]": 'Other/Mixed (vs White)',
        "C(EthnicOrigin_cat, Treatment(reference='White'))[T.Unknown]": 'Unknown ethnicity (vs White)',
        "C(Repair_Type, Treatment(reference='Overlapping'))[T.End to end]": 'End-to-end repair (vs Overlapping)',
        "C(Repair_Type, Treatment(reference='Overlapping'))[T.Other/Unclear]": 'Other repair (vs Overlapping)',
        "C(MOD_ALL, Treatment(reference='SVD'))[T.NBFD]": 'NBFD (vs SVD)',
        "C(MOD_ALL, Treatment(reference='SVD'))[T.VENTOUSE]": 'Ventouse (vs SVD)'
    }
    
    # Run models quietly and collect results
    outcomes = {
        'General Complications': 'Complication_bin',
        'Bowel (Immediate)': 'Bowel_IM_any_issue',
        'Bowel (Long-term)': 'Bowel_LT_any_issue',
        'Urinary (Immediate)': 'Urine_IM_any_issue',
        'Urinary (Long-term)': 'Urine_LT_any_issue',
        'Vaginal (Long-term)': 'Vaginal_LT_any_issue'
    }
    
    results_dict = {}
    
    for outcome_name, outcome_var in outcomes.items():
        try:
            # Create model data
            dfm = df4[[outcome_var] + analytic_predictors].copy()
            dfm = dfm.dropna(subset=[outcome_var])
            dfm["MOD_ALL"] = dfm["MOD_ALL"].replace({"KIELLAND FORCEPS": "NBFD"})
            dfm_full = dfm.dropna(subset=analytic_predictors)
            
            if dfm_full.empty or dfm_full[outcome_var].nunique() < 2:
                continue
                
            formula = (
                f"{outcome_var} ~ "
                "Age_at_deliveryDate + Baby_weight_num + Absence_episiotomy + "
                "C(Parity_Group_v2, Treatment(reference='1')) + "
                "C(EthnicOrigin_cat, Treatment(reference='White')) + "
                "C(Repair_Type, Treatment(reference='Overlapping')) + "
                "C(MOD_ALL, Treatment(reference='SVD'))"
            )
            
            y, X = patsy.dmatrices(formula, data=dfm_full, return_type='dataframe')
            zero_cols = [c for c in X.columns if (X[c].abs().sum() == 0)]
            X = X.drop(columns=zero_cols, errors='ignore')
            X = X.loc[:, ~X.columns.duplicated()]
            
            try:
                model = sm.Logit(y, X).fit(disp=0)
                
                # Extract OR and CI
                params = model.params
                conf = model.conf_int()
                p_values = model.pvalues
                
                or_results = {}
                for param in params.index:
                    if param != 'Intercept':
                        or_val = np.exp(params[param])
                        ci_low = np.exp(conf.loc[param, 0])
                        ci_high = np.exp(conf.loc[param, 1])
                        p_val = p_values[param]
                        
                        # Format with significance
                        if p_val < 0.001:
                            sig = '***'
                        elif p_val < 0.01:
                            sig = '**'
                        elif p_val < 0.05:
                            sig = '*'
                        else:
                            sig = ''
                        
                        or_results[param] = f"{or_val:.2f} ({ci_low:.2f}-{ci_high:.2f}){sig}"
                
                results_dict[outcome_name] = or_results
                
            except:
                # Handle regularized models
                results_dict[outcome_name] = {'Note': 'Regularized model - no CI available'}
                
        except Exception as e:
            results_dict[outcome_name] = {'Error': str(e)}
    
    # Create the summary table
    all_predictors = set()
    for outcome_results in results_dict.values():
        all_predictors.update(outcome_results.keys())
    
    summary_data = []
    for predictor in sorted(all_predictors):
        if predictor in predictor_names:
            row = {'Predictor': predictor_names[predictor]}
        else:
            row = {'Predictor': predictor}
        
        for outcome_name in outcomes.keys():
            if outcome_name in results_dict:
                row[outcome_name] = results_dict[outcome_name].get(predictor, '-')
            else:
                row[outcome_name] = '-'
        
        summary_data.append(row)
    
    regression_table = pd.DataFrame(summary_data)
    
    print("\n--- Table 3: Regression Results Summary ---")
    print("OR (95% CI); *p<0.05, **p<0.01, ***p<0.001")
    print(regression_table.to_string(index=False))
    
    # Save to CSV
    regression_table.to_csv('Table3_Regression_Results.csv', index=False)
    print("\nSaved: Table3_Regression_Results.csv")
    
    return regression_table

# --- 4. Create Visualizations ---
def create_visualizations():
    """Create publication-ready charts."""
    
    # Set up the plotting style
    plt.style.use('default')
    
    # Figure 1: Demographic comparison between 3C and 4th degree tears
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Demographic Characteristics: 3C vs 4th Degree Tears', fontsize=16, fontweight='bold')
    
    # Age distribution
    age_data = pd.DataFrame({
        '3C Tears': df3['Age_Group'].value_counts(normalize=True) * 100,
        '4th Degree Tears': df4['Age_Group'].value_counts(normalize=True) * 100
    }).fillna(0)
    age_data.plot(kind='bar', ax=axes[0,0], title='Age Distribution (%)')
    axes[0,0].set_ylabel('Percentage')
    axes[0,0].tick_params(axis='x', rotation=45)
    
    # BMI distribution
    bmi_data = pd.DataFrame({
        '3C Tears': df3['BMI_Group'].value_counts(normalize=True) * 100,
        '4th Degree Tears': df4['BMI_Group'].value_counts(normalize=True) * 100
    }).fillna(0)
    bmi_data.plot(kind='bar', ax=axes[0,1], title='BMI Distribution (%)')
    axes[0,1].set_ylabel('Percentage')
    axes[0,1].tick_params(axis='x', rotation=45)
    
    # Parity distribution
    parity_data = pd.DataFrame({
        '3C Tears': df3['Parity_Group_v2'].value_counts(normalize=True) * 100,
        '4th Degree Tears': df4['Parity_Group_v2'].value_counts(normalize=True) * 100
    }).fillna(0)
    parity_data.plot(kind='bar', ax=axes[0,2], title='Parity Distribution (%)')
    axes[0,2].set_ylabel('Percentage')
    axes[0,2].tick_params(axis='x', rotation=45)
    
    # Mode of delivery (4th degree only due to missing data in 3C)
    mod_data = df4['MOD_ALL'].value_counts(normalize=True) * 100
    mod_data = mod_data[mod_data > 1]  # Only show categories >1%
    mod_data.plot(kind='bar', ax=axes[1,0], title='Mode of Delivery - 4th Degree (%)')
    axes[1,0].set_ylabel('Percentage')
    axes[1,0].tick_params(axis='x', rotation=45)
    
    # Repair type
    repair_data = pd.DataFrame({
        '3C Tears': df3['Repair_Type'].value_counts(normalize=True) * 100,
        '4th Degree Tears': df4['Repair_Type'].value_counts(normalize=True) * 100
    }).fillna(0)
    repair_data.plot(kind='bar', ax=axes[1,1], title='Repair Type (%)')
    axes[1,1].set_ylabel('Percentage')
    axes[1,1].tick_params(axis='x', rotation=45)
    
    # Complications (4th degree only)
    comp_rates = []
    comp_labels = []
    for comp_type in ['Bowel_IM_any_issue', 'Bowel_LT_any_issue', 'Urine_IM_any_issue', 
                      'Urine_LT_any_issue', 'Vaginal_IM_any_issue', 'Vaginal_LT_any_issue']:
        if comp_type in df4.columns:
            rate = (df4[comp_type] == 1).sum() / df4[comp_type].notna().sum() * 100
            comp_rates.append(rate)
            comp_labels.append(comp_type.replace('_any_issue', '').replace('_', ' '))
    
    axes[1,2].bar(range(len(comp_rates)), comp_rates, color=sns.color_palette("husl", len(comp_rates)))
    axes[1,2].set_title('Complication Rates - 4th Degree (%)')
    axes[1,2].set_ylabel('Percentage')
    axes[1,2].set_xticks(range(len(comp_labels)))
    axes[1,2].set_xticklabels(comp_labels, rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig('Figure1_Demographics.png', dpi=300, bbox_inches='tight')
    print("\nSaved: Figure1_Demographics.png")
    plt.show()

# --- 5. Forest Plot for General Complications ---
def create_forest_plot():
    """Create a forest plot for the general complications model."""
    
    # Use the existing general complications model
    dfm = df4[['Complication_bin'] + analytic_predictors].copy()
    dfm = dfm.dropna(subset=['Complication_bin'])
    dfm["MOD_ALL"] = dfm["MOD_ALL"].replace({"KIELLAND FORCEPS": "NBFD"})
    dfm_full = dfm.dropna(subset=analytic_predictors)
    
    formula = (
        "Complication_bin ~ "
        "Age_at_deliveryDate + Baby_weight_num + Absence_episiotomy + "
        "C(Parity_Group_v2, Treatment(reference='1')) + "
        "C(EthnicOrigin_cat, Treatment(reference='White')) + "
        "C(Repair_Type, Treatment(reference='Overlapping')) + "
        "C(MOD_ALL, Treatment(reference='SVD'))"
    )
    
    try:
        y, X = patsy.dmatrices(formula, data=dfm_full, return_type='dataframe')
        zero_cols = [c for c in X.columns if (X[c].abs().sum() == 0)]
        X = X.drop(columns=zero_cols, errors='ignore')
        X = X.loc[:, ~X.columns.duplicated()]
        
        model = sm.Logit(y, X).fit(disp=0)
        
        # Extract results (excluding intercept)
        params = model.params.drop('Intercept', errors='ignore')
        conf = model.conf_int().drop('Intercept', errors='ignore')
        p_values = model.pvalues.drop('Intercept', errors='ignore')
        
        # Create forest plot data
        forest_data = pd.DataFrame({
            'OR': np.exp(params),
            'CI_lower': np.exp(conf.iloc[:, 0]),
            'CI_upper': np.exp(conf.iloc[:, 1]),
            'p_value': p_values
        })
        
        # Clean up predictor names
        predictor_names_clean = {
            'Age_at_deliveryDate': 'Age (per year)',
            'Baby_weight_num': 'Baby weight (per kg)',
            'Absence_episiotomy': 'No episiotomy',
            "C(Parity_Group_v2, Treatment(reference='1'))[T.<1]": 'Parity <1 (vs 1)',
            "C(Parity_Group_v2, Treatment(reference='1'))[T.2]": 'Parity 2 (vs 1)',
            "C(EthnicOrigin_cat, Treatment(reference='White'))[T.South Asian]": 'South Asian ethnicity',
            "C(EthnicOrigin_cat, Treatment(reference='White'))[T.Black]": 'Black ethnicity',
            "C(EthnicOrigin_cat, Treatment(reference='White'))[T.Other/Mixed]": 'Other/Mixed ethnicity',
            "C(EthnicOrigin_cat, Treatment(reference='White'))[T.Unknown]": 'Unknown ethnicity',
            "C(Repair_Type, Treatment(reference='Overlapping'))[T.End to end]": 'End-to-end repair',
            "C(Repair_Type, Treatment(reference='Overlapping'))[T.Other/Unclear]": 'Other repair type',
            "C(MOD_ALL, Treatment(reference='SVD'))[T.NBFD]": 'NBFD delivery',
            "C(MOD_ALL, Treatment(reference='SVD'))[T.VENTOUSE]": 'Ventouse delivery'
        }
        
        forest_data.index = [predictor_names_clean.get(idx, idx) for idx in forest_data.index]
        forest_data = forest_data.sort_values('OR')
        
        # Create the forest plot
        fig, ax = plt.subplots(figsize=(12, 8))
        
        y_pos = range(len(forest_data))
        
        # Plot points and error bars
        colors = ['red' if p < 0.05 else 'black' for p in forest_data['p_value']]
        
        for i, (idx, row) in enumerate(forest_data.iterrows()):
            ax.errorbar(row['OR'], i, 
                       xerr=[[row['OR'] - row['CI_lower']], [row['CI_upper'] - row['OR']]],
                       fmt='o', color=colors[i], capsize=5, capthick=2, markersize=8)
        
        # Add vertical line at OR = 1
        ax.axvline(x=1, color='gray', linestyle='--', alpha=0.7)
        
        # Formatting
        ax.set_yticks(y_pos)
        ax.set_yticklabels(forest_data.index)
        ax.set_xlabel('Odds Ratio (95% CI)', fontsize=12)
        ax.set_title('Risk Factors for General Complications in 4th Degree Tears', fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)
        
        # Add legend
        from matplotlib.lines import Line2D
        legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8, label='p < 0.05'),
                          Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=8, label='p ≥ 0.05')]
        ax.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig('Figure2_Forest_Plot_General_Complications.png', dpi=300, bbox_inches='tight')
        print("\nSaved: Figure2_Forest_Plot_General_Complications.png")
        plt.show()
        
    except Exception as e:
        print(f"Could not create forest plot: {e}")

# --- 6. Complications Heatmap ---
def create_complications_heatmap():
    """Create a heatmap showing complication rates by repair type."""
    
    # Prepare data for heatmap
    comp_by_repair = []
    
    for repair_type in ['Overlapping', 'End to end']:
        subset = df4[df4['Repair_Type'] == repair_type]
        row_data = {'Repair Type': repair_type}
        
        for comp_type in ['Bowel_IM_any_issue', 'Bowel_LT_any_issue', 'Urine_IM_any_issue', 
                          'Urine_LT_any_issue', 'Vaginal_IM_any_issue', 'Vaginal_LT_any_issue']:
            if comp_type in subset.columns:
                rate = (subset[comp_type] == 1).sum() / subset[comp_type].notna().sum() * 100
                row_data[comp_type.replace('_any_issue', '').replace('_', ' ')] = rate
            else:
                row_data[comp_type.replace('_any_issue', '').replace('_', ' ')] = 0
        
        comp_by_repair.append(row_data)
    
    heatmap_df = pd.DataFrame(comp_by_repair).set_index('Repair Type')
    
    # Create heatmap
    plt.figure(figsize=(10, 6))
    sns.heatmap(heatmap_df, annot=True, fmt='.1f', cmap='Reds', 
                cbar_kws={'label': 'Complication Rate (%)'})
    plt.title('Complication Rates by Repair Type (%)', fontsize=14, fontweight='bold')
    plt.ylabel('Repair Type')
    plt.xlabel('Complication Type')
    plt.tight_layout()
    plt.savefig('Figure3_Complications_Heatmap.png', dpi=300, bbox_inches='tight')
    print("\nSaved: Figure3_Complications_Heatmap.png")
    plt.show()

# Generate all outputs
create_demographic_table()
create_complications_table()
create_regression_summary_table()
create_visualizations()
create_forest_plot()
create_complications_heatmap()

print("\n===================================================")
print("ALL PUBLICATION OUTPUTS GENERATED")
print("===================================================")
print("\nFiles created:")
print("- Table1_Demographics.csv")
print("- Table2_Complications.csv") 
print("- Table3_Regression_Results.csv")
print("- Figure1_Demographics.png")
print("- Figure2_Forest_Plot_General_Complications.png")
print("- Figure3_Complications_Heatmap.png")


 # =============================================================================
 # PART THREE: Comparative Statistics
 # =============================================================================

from scipy.stats import chi2_contingency, fisher_exact

print("\n\n===================================================")
print("PART THREE: Comparative Statistics")
print("===================================================")


# --- Helper Function for Statistical Tests ---
def run_comparison_test(df, group_var, complication_var, title):
    """
    Performs a Chi-squared or Fisher's exact test and prints the results.
    """
    print(f"\n--- {title} ---")
    
    # Create a contingency table, dropping any rows with missing data in the relevant columns
    contingency_table = pd.crosstab(df[group_var], df[complication_var].dropna())
    
    # Check if the table is valid for a test
    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:
        print("Cannot perform test: table is not 2x2 or larger (not enough variation in data).")
        print(contingency_table)
        return

    print("Contingency Table:")
    print(contingency_table)

    # Decide which test to use
    # If any expected frequency is < 5, use Fisher's. Otherwise, use Chi-squared.
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    
    if (expected < 5).any():
        print("\nNote: Using Fisher's Exact Test due to low expected cell counts.")
        odds_ratio, p_value = fisher_exact(contingency_table)
        test_name = "Fisher's Exact Test"
    else:
        p_value = p
        test_name = "Chi-squared Test"

    print(f"\nTest: {test_name}")
    print(f"P-value: {p_value:.4f}")

    if p_value < 0.05:
        print("Result: The difference is statistically significant (p < 0.05).")
    else:
        print("Result: The difference is not statistically significant (p >= 0.05).")


# --- Note on Questions A, B, C ---
print("\n--- Analysis for Questions A, B, C (3c vs 4th degree tears) ---")
print("NOTE: A direct comparison of immediate and late complications between 3c and 4th degree tears")
print("is not possible because the detailed complication data is only available for the 4th degree tear dataset (df4).\n")


# --- Analysis for Questions D, E, F (End-to-end vs Overlapping repair in df4) ---

# 1. Prepare the data: Create aggregate complication flags
df_comp = df4.copy()

# Filter to only the two repair types we want to compare
df_comp = df_comp[df_comp['Repair_Type'].isin(['Overlapping', 'End to end'])]

# Define complication columns for each category
bowel_imm = ['Bowel (immediate issues)_Faecal incontinence', 'Bowel (immediate issues)_Flatus incontinence', 'Bowel (immediate issues)_Urgency of stool']
bowel_lt = ['Bowel (long term issues)_Faecal incontinence', 'Bowel (long term issues)_Flatus incontinence', 'Bowel (long term issues)_Urgency']
urinary_imm = ['Urinary problems (immediate issues)_Urgency', 'Urinary problems (immediate issues)_Frequency', 'Urinary problems (immediate issues)_Leakage', 'Urinary problems (immediate issues)_Leakage on strenuous activity', 'Urinary problems (immediate issues)_Voiding dysfunction']
urinary_lt = ['Urinary problems (long term issues)_Urgency', 'Urinary problems (long term issues)_Frequency', 'Urinary problems (long term issues)_Leakage', 'Urinary problems (long term issues)_Leakage on strenuous activity', 'Urinary problems (long term issues)_Voiding dysfunction']
vaginal_imm = ['Vaginal problems (immediate)_Body image', 'Vaginal problems (immediate)_Dyspareunia', 'Vaginal problems (immediate)_Vaginal lump']
vaginal_lt = ['Vaginal problems (long term)_Body image', 'Vaginal problems (long term)_Dyspareunia', 'Vaginal problems (long term)_Vaginal lump']

# Helper to create 'any complication' flag
def create_any_comp_flag(df, columns):
    # Convert 'Y' to 1, everything else to 0, then sum across rows
    df_numeric = df[columns].apply(lambda x: x.str.strip().str.upper() == 'Y').astype(int)
    return (df_numeric.sum(axis=1) > 0).astype(int)

# Special handling for 'Able to defer bowels' where 'N' is the symptom
df_comp['Bowel_imm_defer_comp'] = (df_comp['Bowel (immediate issues)_Able to defer bowels?'].str.strip().str.upper() == 'N').astype(int)
df_comp['Bowel_lt_defer_comp'] = (df_comp['Bowel (long term issues)_Able to defer bowels?'].str.strip().str.upper() == 'N').astype(int)

# Create the aggregate flags
df_comp['Any_Bowel_Immediate'] = ((create_any_comp_flag(df_comp, bowel_imm) + df_comp['Bowel_imm_defer_comp']) > 0).astype(int)
df_comp['Any_Bowel_Late'] = ((create_any_comp_flag(df_comp, bowel_lt) + df_comp['Bowel_lt_defer_comp']) > 0).astype(int)
df_comp['Any_Urinary_Immediate'] = create_any_comp_flag(df_comp, urinary_imm)
df_comp['Any_Urinary_Late'] = create_any_comp_flag(df_comp, urinary_lt)
df_comp['Any_Vaginal_Immediate'] = create_any_comp_flag(df_comp, vaginal_imm)
df_comp['Any_Vaginal_Late'] = create_any_comp_flag(df_comp, vaginal_lt)


# 2. Run the comparisons

# D. Bowel complications by repair type
run_comparison_test(df_comp, 'Repair_Type', 'Any_Bowel_Immediate', 'D: Immediate Bowel Complications (End-to-end vs Overlapping)')
run_comparison_test(df_comp, 'Repair_Type', 'Any_Bowel_Late', 'D: Late Bowel Complications (End-to-end vs Overlapping)')

# E. Urinary complications by repair type
run_comparison_test(df_comp, 'Repair_Type', 'Any_Urinary_Immediate', 'E: Immediate Urinary Complications (End-to-end vs Overlapping)')
run_comparison_test(df_comp, 'Repair_Type', 'Any_Urinary_Late', 'E: Late Urinary Complications (End-to-end vs Overlapping)')

# F. Vaginal complications by repair type
run_comparison_test(df_comp, 'Repair_Type', 'Any_Vaginal_Immediate', 'F: Immediate Vaginal Complications (End-to-end vs Overlapping)')
run_comparison_test(df_comp, 'Repair_Type', 'Any_Vaginal_Late', 'F: Late Vaginal Complications (End-to-end vs Overlapping)')


